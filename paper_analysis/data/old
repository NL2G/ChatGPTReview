	paperId	title	abstract	venue	authors
0	cf1f26e7cbed3958b3c2870656568c299fece6e3	Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models	We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, even clinical decision-making.	medRxiv	[{'authorId': '2051250388', 'name': 'T. Kung'}, {'authorId': '2045482362', 'name': 'M. Cheatham'}, {'authorId': '2196935808', 'name': 'A. Medinilla'}, {'authorId': '2196932745', 'name': 'chatGPT'}, {'authorId': '2196932743', 'name': 'C. Sillos'}, {'authorId': '2194457777', 'name': 'L. D. De Leon'}, {'authorId': '2196937260', 'name': 'C. Elepano'}, {'authorId': '2101601654', 'name': 'M. Madriaga'}, {'authorId': '117973505', 'name': 'R. Aggabao'}, {'authorId': '2196935806', 'name': 'G. Diaz-Candido'}, {'authorId': '7262505', 'name': 'J. Maningo'}, {'authorId': '2196932802', 'name': 'V. Tseng'}]
1	a45ff64e22f8005d563fed1ee545ddc98aab9766	AI bot ChatGPT writes smart essays - should professors worry?		Nature	[{'authorId': '1413934873', 'name': 'Chris Stokel-Walker'}]
2	8822357efe500caded16e603d21239be3a39547c	ChatGPT: The End of Online Exam Integrity?	,	ArXiv	[{'authorId': '2656889', 'name': 'Teo Susnjak'}]
3	3d3012bfcc8bc7e4dc84c177e94650e66f03bc5b	Are ChatGPT and AlphaCode going to replace programmers?		Nature	[{'authorId': '4596241', 'name': 'D. Castelvecchi'}]
4	7d4867e28b02059eef4cb25bfcd304b2071b30a9	How Well Does ChatGPT Do When Taking the Medical Licensing Exams? The Implications of Large Language Models for Medical Education and Knowledge Assessment	Background: ChatGPT is a 175 billion parameter natural language processing model which can generate conversation style responses to user input. Objective: To evaluate the performance of ChatGPT on questions within the scope of United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams, as well as analyze responses for user interpretability. Methods: We used two novel sets of multiple choice questions to evaluate ChatGPT's performance, each with questions pertaining to Step 1 and Step 2. The first was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the userbase. The second, was the National Board of Medical Examiners (NBME) Free 120-question exams. After prompting ChatGPT with each question, ChatGPT's selected answer was recorded, and the text output evaluated across three qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results: On the four datasets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free- Step2, ChatGPT achieved accuracies of 44%, 42%, 64.4%, and 57.8%. The model demonstrated a significant decrease in performance as question difficulty increased (P=.012) within the AMBOSS- Step1 dataset. We found logical justification for ChatGPT's answer selection was present in 100% of outputs. Internal information to the question was present in >90% of all questions. The presence of information external to the question was respectively 54.5% and 27% lower for incorrect relative to correct answers on the NBME-Free-Step1 and NBME-Free-Step2 datasets (P<=.001). Conclusion: ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at greater than 60% threshold on the NBME-Free- Step-1 dataset we show that the model is comparable to a third year medical student. Additionally, due to the dialogic nature of the response to questions, we demonstrate ChatGPT's ability to provide reasoning and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as a medical education tool.	medRxiv	[{'authorId': '2107029904', 'name': 'A. Gilson'}, {'authorId': '40902199', 'name': 'C. Safranek'}, {'authorId': '2181652492', 'name': 'T. Huang'}, {'authorId': '11035690', 'name': 'V. Socrates'}, {'authorId': '2087060121', 'name': 'L. Chi'}, {'authorId': '143794498', 'name': 'R. A. Taylor'}, {'authorId': '3181480', 'name': 'David Chartash'}]
5	b36acdfc67612d707c95d1ed282672d3ca262be7	Comparing scientific abstracts generated by ChatGPT to original abstracts using an artificial intelligence output detector, plagiarism detector, and blinded human reviewers	Background Large language models such as ChatGPT can produce increasingly realistic text, with unknown information on the accuracy and integrity of using these models in scientific writing. Methods We gathered ten research abstracts from five high impact factor medical journals (n=50) and asked ChatGPT to generate research abstracts based on their titles and journals. We evaluated the abstracts using an artificial intelligence (AI) output detector, plagiarism detector, and had blinded human reviewers try to distinguish whether abstracts were original or generated. Results All ChatGPT-generated abstracts were written clearly but only 8% correctly followed the specific journal’s formatting requirements. Most generated abstracts were detected using the AI output detector, with scores (higher meaning more likely to be generated) of median [interquartile range] of 99.98% [12.73, 99.98] compared with very low probability of AI-generated output in the original abstracts of 0.02% [0.02, 0.09]. The AUROC of the AI output detector was 0.94. Generated abstracts scored very high on originality using the plagiarism detector (100% [100, 100] originality). Generated abstracts had a similar patient cohort size as original abstracts, though the exact numbers were fabricated. When given a mixture of original and general abstracts, blinded human reviewers correctly identified 68% of generated abstracts as being generated by ChatGPT, but incorrectly identified 14% of original abstracts as being generated. Reviewers indicated that it was surprisingly difficult to differentiate between the two, but that the generated abstracts were vaguer and had a formulaic feel to the writing. Conclusion ChatGPT writes believable scientific abstracts, though with completely generated data. These are original without any plagiarism detected but are often identifiable using an AI output detector and skeptical human reviewers. Abstract evaluation for journals and medical conferences must adapt policy and practice to maintain rigorous scientific standards; we suggest inclusion of AI output detectors in the editorial process and clear disclosure if these technologies are used. The boundaries of ethical and acceptable use of large language models to help scientific writing remain to be determined.	bioRxiv	[{'authorId': '2149285611', 'name': 'Catherine A. Gao'}, {'authorId': '1737779229', 'name': 'Frederick M. Howard'}, {'authorId': '1411115878', 'name': 'N. Markov'}, {'authorId': '2186065223', 'name': 'E. Dyer'}, {'authorId': '144972037', 'name': 'S. Ramesh'}, {'authorId': '1683396', 'name': 'Yuan Luo'}, {'authorId': '2198493546', 'name': 'Alexander T. Pearson'}]
6	7eb094b63ecd57b530a9ccfbfd96287a510b9c7f	"""I think this is the most disruptive technology"": Exploring Sentiments of ChatGPT Early Adopters using Twitter Data"	—Large language models have recently attracted sig- niﬁcant attention due to their impressive performance on a variety of tasks. ChatGPT developed by OpenAI is one such implementation of a large, pre-trained language model that has gained immense popularity among early adopters, where certain users go to the extent of characterizing it as a disruptive technology in many domains. Understanding such early adopters’ sentiments is important because it can provide insights into the potential success or failure of the technology, as well as its strengths and weaknesses. In this paper, we conduct a mixed-method study using 10,732 tweets from early ChatGPT users. We ﬁrst use topic modelling to identify the main topics and then perform an in-depth qualitative sentiment analysis of each topic. Our results show that the majority of the early adopters have expressed overwhelmingly positive sentiments related to topics such as Disruptions to software development , Entertainment and exercising creativity . Only a limited percentage of users expressed concerns about issues such as the potential for misuse of ChatGPT, especially regarding topics such as Impact on educational aspects . We discuss these ﬁndings by providing speciﬁc examples for each topic and then detail implications related to addressing these concerns for both researchers and users.	ArXiv	[{'authorId': '3367165', 'name': 'Mubin Ul Haque'}, {'authorId': '39967996', 'name': 'I. Dharmadasa'}, {'authorId': '3375832', 'name': 'Zarrin Tasnim Sworna'}, {'authorId': '1678246762', 'name': 'R. Rajapakse'}, {'authorId': '2195183752', 'name': 'Hussain Ahmad'}]
7	23cae400cfd1a7c455c721256b838e98a307d5e6	ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports	The release of ChatGPT, a language model capable of generating text that appears human-like and authentic, has gained significant attention beyond the research community. We expect that the convincing performance of ChatGPT incentivizes users to apply it to a variety of downstream tasks, including prompting the model to simplify their own medical reports. To investigate this phenomenon, we conducted an exploratory case study. In a questionnaire, we asked 15 radiologists to assess the quality of radiology reports simplified by ChatGPT. Most radiologists agreed that the simplified reports were factually correct, complete, and not potentially harmful to the patient. Nevertheless, instances of incorrect statements, missed key medical findings, and potentially harmful passages were reported. While further studies are needed, the initial insights of this study indicate a great potential in using large language models like ChatGPT to improve patient-centered care in radiology and other medical domains.	ArXiv	[{'authorId': '2199013582', 'name': 'Katharina Jeblick'}, {'authorId': '103583046', 'name': 'B. Schachtner'}, {'authorId': '2051875796', 'name': 'Jakob Dexl'}, {'authorId': '1405351735', 'name': 'Andreas Mittermeier'}, {'authorId': '2158583540', 'name': 'Anna Theresa Stüber'}, {'authorId': '2124700239', 'name': 'Johanna Topalis'}, {'authorId': '2053297121', 'name': 'Tobias Weber'}, {'authorId': '119656907', 'name': 'P. Wesp'}, {'authorId': '31463178', 'name': 'B. Sabel'}, {'authorId': '2566938', 'name': 'J. Ricke'}, {'authorId': '4925703', 'name': 'M. Ingrisch'}]
8	535c8840cc00d83e3448bffa0f0dd8d05da1bc79	OpenAI ChatGPT Generated Literature Review: Digital Twin in Healthcare		SSRN Electronic Journal	[{'authorId': '30528739', 'name': 'Ömer Aydın'}, {'authorId': '40631192', 'name': 'Enis Karaarslan'}]
9	c3d4471f095d37549fef3f824a2057f7497bc36e	How would Stance Detection Techniques Evolve after the Launch of ChatGPT?	Stance detection refers to the task of extract-ing the standpoint (Favor, Against or Neither) towards a target in given texts. Such research gains increasing attention with the pro-liferation of social media contents. The conventional framework of handling stance detection is converting it into text classiﬁcation tasks. Deep learning models have already re-placed rule-based models and traditional machine learning models in solving such problems. Current deep neural networks are fac-ing two main challenges which are insufﬁ-cient labeled data and information in social media posts and the unexplainable nature of deep learning models. A new pre-trained language model chatGPT was launched on Nov 30, 2022. For the stance detection tasks, our experiments show that ChatGPT can achieve SOTA or similar performance for commonly used datasets including SemEval-2016 and P-Stance. At the same time, ChatGPT can provide explanation for its own prediction, which is beyond the capability of any existing model. The explanations for the cases it cannot provide classiﬁcation results are especially useful. ChatGPT has the potential to be the best AI model for stance detection tasks in NLP, or at least change the research paradigm of this ﬁeld. ChatGPT also opens up the possibility of building explanatory AI for stance detection.	ArXiv	[{'authorId': '3047890', 'name': 'Bowen Zhang'}, {'authorId': '2199013050', 'name': 'Daijun Ding'}, {'authorId': '2147240380', 'name': 'Liwen Jing'}]
10	c4461a7160c54daf064a3091c785ab882e0ca736	Framework for A Foreign Language Teaching Software for Children Utilizing AR, Voicebots and ChatGPT (Large Language Models)	The cognitive capabilities of children develop during the early years of their life. Research shows that learning a foreign language helps develop cognitive skills. Moreover, learning a foreign language has become essential and an increasing number of parents would like their kids to start learning a foreign language at an early age. However, engaging little kids with learning activities is challenging. In this study, we propose a framework for developing a language learning software tool utilizing Augmented Reality (AR), Voicebots, and ChatGPT (an AI utilizing the Large Language Model) technologies to provide a unique product for small kids to teach a foreign language. With AR and Voicebots, the product will grab attention, motivate and provide an entertaining learning environment. The capabilities of ChatGPT will be utilized to efficiently prepare the content for the software tool. We utilize the capabilities of ChatGPT to generate interactive dialogs that will be hosted at Google DialogFlow. We believe the framework and the design principles we propose in this study can be a blueprint for developing highly effective foreign language teaching software.	The journal of cognitive systems	[{'authorId': '120795838', 'name': 'Oguzhan Topsakal'}, {'authorId': '90799265', 'name': 'Elif Topsakal'}]
11	e033faeef032e2c15f1fff5bf54cbc2998f33514	ChatGPT y el futuro de la comunicación científica	ChatGPT es un modelo de lenguaje desarrollado por OpenAI que utiliza la técnica de procesamiento de lenguaje natural (NLP) de transformación autorregresiva (Transformer) para generar respuestas coherentes y naturales a preguntas o comentarios en tiempo real. Una forma en la que ChatGPT podría ser útil en la comunicación científica es como una herramienta para ayudar a comunicar las investigaciones de manera más clara y accesible para el público en general. Otra forma en la que esta inteligencia artificial (IA) podría ser útil es como una herramienta para contribuir a los científicos y académicos a mantenerse al día con las últimas investigaciones y desarrollos en su campo de trabajo. Especialmente el ChatGPT podría ser utilizado para recopilar y resumir artículos científicos y otras publicaciones relevantes de manera automatizada, lo que podría ayudar a los científicos a ahorrar tiempo y esfuerzo al tener que leer y analizar cada artículo por sí mismos. Un elemento crucial en algunos tipos de estudios, la síntesis de literatura, sea para una revisión narrativa o panorámica, el uso de ChatGPT permitiría generar resúmenes o abstracts de investigaciones de manera más eficiente, o incluso a redactar secciones de un artículo científico que requieran menos análisis crítico o interpretación. Sin embargo, la escritura de un artículo científico completo requiere un conocimiento profundo del campo de investigación y la habilidad para analizar y sintetizar datos de manera crítica. Esto es algo que solo puede hacerse de manera efectiva a través del trabajo y el esfuerzo de un ser humano.	Metaverse Basic and Applied Research	[{'authorId': '2102953588', 'name': 'William Castillo-González'}]
12	8422946a71cc8aba3d148a9844ebc3b29b4a5d5a	Daily briefing: Will ChatGPT kill the essay assignment?		Nature	[{'authorId': '82335473', 'name': 'Flora Graham'}]
13	6fd85727e91ded0f66c9862b1677b62f2b2a4375	ChatGPT User Experience: Implications for Education		SSRN Electronic Journal	[{'authorId': '152475512', 'name': 'X. Zhai'}]
14	e5c3c6ef80b018b52baedebe4a7249d335e2c337	Performing Effective Research Using ChatGPT		Indian Journal of Computer Science	[{'authorId': '65991448', 'name': 'Subhabaha Pal'}]
15	6f4ec89b79e2f0aee9dc2bfc1b0043cc8a4342db	Smarter than Humans? Validating how OpenAI’s ChatGPT Model Explains Crowdfunding, Alternative Finance and Community Finance		SSRN Electronic Journal	[{'authorId': '100569753', 'name': 'K. Wenzlaff'}, {'authorId': '1749307', 'name': 'S. Spaeth'}]
16	1a1568f416a6a40a55b6671a7f5cb779677c25a0	A inteligência artificial e eu: escrevendo o editorial juntamente com o ChatGPT		Revista Eletrônica de Ciência Administrativa	[{'authorId': '51056140', 'name': 'Luciano Rossoni'}, {'authorId': '2202433509', 'name': 'Chat Gpt'}]
17	f85cd06395716f4debbfabf1eb503decebd3ff02	Exploring the Role of Artificial Intelligence in Enhancing Academic Performance: A Case Study of ChatGPT		SSRN Electronic Journal	[{'authorId': '2041888108', 'name': 'M. M. Alshater'}]
18	eddfb9be78cfe94193766e3722eb0e56c3d24cef	ChatGPT is fun, but not an author	In less than 2 months, the artificial intelligence (AI) program ChatGPT has become a cultural sensation. It is freely accessible through a web portal created by the tool’s developer, OpenAI. The program—which automatically creates text based on written prompts—is so popular that it’s likely to be “at capacity right now” if you attempt to use it. When you do get through, ChatGPT provides endless entertainment. I asked it to rewrite the first scene of the classic American play Death of a Salesman, but to feature Princess Elsa from the animated movie Frozen as the main character instead of Willy Loman. The output was an amusing conversation in which Elsa—who has come home from a tough day of selling—is told by her son Happy, “Come on, Mom. You’re Elsa from Frozen. You have ice powers and you’re a queen. You’re unstoppable.” Mash-ups like this are certainly fun, but there are serious implications for generative AI programs like ChatGPT in science and academia.	Science	[{'authorId': '2003404994', 'name': 'H. Thorp'}]
19	1f22de83d912176cb8857efa1c6d65b14d6a2f5c	ChatGPT is not all you need. A State of the Art Review of large Generative AI models	. During the last two years there has been a plethora of large generative models such as ChatGPT or Stable Diﬀusion that have been published. Concretely, these models are able to perform tasks such as being a general question and answering system or automatically creating artistic images that are revolutionizing several sectors. Consequently, the implications that these generative models have in the industry and society are enormous, as several job positions may be transformed. For example, Generative AI is capable of transforming eﬀectively and cre-atively texts to images, like the DALLE-2 model; text to 3D images, like the Dreamfusion model; images to text, like the Flamingo model; texts to video, like the Phenaki model; texts to audio, like the AudioLM model; texts to other texts, like ChatGPT; texts to code, like the Codex model; texts to scientiﬁc texts, like the Galactica model or even create algorithms like AlphaTensor. This work consists on an attempt to describe in a concise way the main models are sectors that are aﬀected by generative AI and to provide a taxonomy of the main generative models published recently.	ArXiv	[{'authorId': '2200162353', 'name': 'Roberto Gozalo-Brizuela'}, {'authorId': '1398645148', 'name': 'E.C. Garrido-Merchán'}]
20	0570e8fc8b02e7eb66e798b00726fba0592ea90f	ChatGPT listed as author on research papers: many scientists disapprove		Nature	[{'authorId': '1413934873', 'name': 'Chris Stokel-Walker'}]
21	0d25a53184a9c56084416b292de9a8fef4b27347	Tools such as ChatGPT threaten transparent science; here are our ground rules for their use		Nature	[]
22	50aea7bae4c478e850f218e58da0e24f501ab8fc	Abstracts written by ChatGPT fool scientists		Nature	[{'authorId': '40898374', 'name': 'Holly Else'}]
23	9dafa6c5c609348b46734fc8997b93b3587fec6e	Collaborating With ChatGPT: Considering the Implications of Generative Artificial Intelligence for Journalism and Media Education	Generative artificial intelligence (AI) is ushering in an era of potential transformation of journalism and media content. This essay considers one notable generative AI platform called ChatGPT made available to the public in 2022 for free use. ChatGPT allows users to enter text prompts and rapidly generates text responses drawn from its knowledge acquired via machine learning in engagement with the internet. This essay is coauthored by a human journalism and media professor in collaboration with ChatGPT. The essay demonstrates the capacity and limitations of ChatGPT and offers reflections on the implications of generative AI for journalism and media education.	Journalism &amp; Mass Communication Educator	[{'authorId': '1795976', 'name': 'J. Pavlik'}]
24	8f64f4633d9c482bb826b7a9fe9c1493837d7112	Is ChatGPT A Good Translator? A Preliminary Study	This report provides a preliminary evaluation of ChatGPT for machine translation, including translation prompt, multilingual translation, and translation robustness. We adopt the prompts advised by ChatGPT to trigger its translation ability and ﬁnd that the candidate prompts generally work well and show minor performance differences. By evaluating on a number of benchmark test sets 1 , we ﬁnd that ChatGPT performs competitively with commercial translation products (e.g., Google Translate) on high-resource European languages but lags behind signiﬁcantly on low-resource or distant languages. For distant languages, we explore an interesting strategy named pivot prompting that asks ChatGPT to translate the source sentence into a high-resource pivot language before into the target language, which improves the translation performance signiﬁcantly. As for the translation robustness, ChatGPT does not perform as well as the commercial systems on biomedical abstracts or Reddit comments but is potentially a good translator for spoken language.	ArXiv	[{'authorId': '12386833', 'name': 'Wenxiang Jiao'}, {'authorId': '2144328160', 'name': 'Wenxuan Wang'}, {'authorId': '2161306685', 'name': 'Jen-tse Huang'}, {'authorId': '2144800839', 'name': 'Xing Wang'}, {'authorId': '2909321', 'name': 'Zhaopeng Tu'}]
25	9f530ebf624bf58e91b2a1f20b0799a45ca48f9a	An Analysis of the Automatic Bug Fixing Performance of ChatGPT	—To support software developers in ﬁnding and ﬁxing software bugs, several automated program repair techniques have been introduced. Given a test suite, standard methods usually either synthesize a repair, or navigate a search space of software edits to ﬁnd test-suite passing variants. Recent program repair methods are based on deep learning approaches. One of these novel methods, which is not primarily intended for automated program repair, but is still suitable for it, is ChatGPT. The bug ﬁxing performance of ChatGPT, however, is so far unclear. Therefore, in this paper we evaluate ChatGPT on the standard bug ﬁxing benchmark set, QuixBugs, and compare the perfor- mance with the results of several other approaches reported in the literature. We ﬁnd that ChatGPT’s bug ﬁxing performance is competitive to the common deep learning approaches CoCoNut and Codex and notably better than the results reported for the standard program repair approaches. In contrast to previous approaches, ChatGPT offers a dialogue system through which further information, e.g., the expected output for a certain input or an observed error message, can be entered. By providing such hints to ChatGPT, its success rate can be further increased, ﬁxing 31 out of 40 bugs, outperforming state-of-the-art.	ArXiv	[{'authorId': '51040229', 'name': 'D. Sobania'}, {'authorId': '2107692276', 'name': 'Martin Briesch'}, {'authorId': '2202175551', 'name': 'Carol Hanna'}, {'authorId': '1923360', 'name': 'J. Petke'}]
26	8cf06d326bda0d619316a346de75cc34b6a981a3	Chatbots, ChatGPT, and Scholarly Manuscripts WAME Recommendations on ChatGPT and Chatbots in Relation to Scholarly Publications	Journals have begun to publish papers in which chatbots such as ChatGPT are shown as co-authors. The following WAME recommendations are intended to inform editors and help them develop policies regarding chatbots for their journals, to help authors understand how use of chatbots might be attributed in their work, and address the need for all journal editors to have access manuscript screening tools. In this rapidly evolving field, we expect these recommendations to evolve as well.	Afro-Egyptian Journal of Infectious and Endemic Diseases	[{'authorId': '2204181382', 'name': 'Chris Zielinski'}, {'authorId': '2204229179', 'name': 'Margaret Winker'}, {'authorId': '2204060826', 'name': 'Rakesh Aggarwal'}, {'authorId': '2204139714', 'name': 'Lorraine Ferris'}, {'authorId': '2204088680', 'name': 'Markus Heinemann'}, {'authorId': None, 'name': 'Jose Florencio Lapeña, Jr'}, {'authorId': '2204241478', 'name': 'Sanjay Pai'}, {'authorId': '2204225725', 'name': 'Edsel Ing'}, {'authorId': '2204240490', 'name': 'Leslie Citrome'}]
27	3c76d5b253a3f8f2bb2294e94037b6fe16bf1ab1	Are ChatGPT's knowledge and interpretation ability comparable to those of medical students in Korea for taking a parasitology examination?: a descriptive study.	This study aimed to compare the knowledge and interpretation ability of ChatGPT, a language model of artificial general intelligence, with those of medical students in Korea by administering a parasitology examination to both ChatGPT and medical students. The examination consisted of 79 items and was administered to ChatGPT on January 1, 2023. The examination results were analyzed in terms of ChatGPT's overall performance score, its correct answer rate by the items' knowledge level, and the acceptability of its explanations of the items. ChatGPT's performance was lower than that of the medical students, and ChatGPT's correct answer rate was not related to the items' knowledge level. However, there was a relationship between acceptable explanations and correct answers. In conclusion, ChatGPT's knowledge and interpretation ability for this parasitology examination were not yet comparable to those of medical students in Korea.	Journal of Educational Evaluation for Health Professions	[{'authorId': '143918822', 'name': 'Sun Huh'}]
28	5e2921320cc6fa64d3eb5f3c6c4087e5fcbb9c8f	A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity	"This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 21 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 64.33% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation, in a multi-turn""prompt engineering""fashion."		[{'authorId': '23672613', 'name': 'Yejin Bang'}, {'authorId': '2107033245', 'name': 'Samuel Cahyawijaya'}, {'authorId': '40221187', 'name': 'Nayeon Lee'}, {'authorId': '47653392', 'name': 'Wenliang Dai'}, {'authorId': '144610224', 'name': 'Dan Su'}, {'authorId': '150048491', 'name': 'Bryan Wilie'}, {'authorId': '116344405', 'name': 'Holy Lovenia'}, {'authorId': '3391272', 'name': 'Ziwei Ji'}, {'authorId': '1660855299', 'name': 'Tiezheng Yu'}, {'authorId': '2160716372', 'name': 'Willy Chung'}, {'authorId': '2187874252', 'name': 'Quyet V. Do'}, {'authorId': '98271906', 'name': 'Yan Xu'}, {'authorId': '2057151752', 'name': 'Pascale Fung'}]
29	fdbe8d1896b754125c6255f07bd9fecc2ea59127	Putting ChatGPT's Medical Advice to the (Turing) Test	Importance: Chatbots could play a role in answering patient questions, but patients' ability to distinguish between provider and chatbot responses, and patients' trust in chatbots' functions are not well established. Objective: To assess the feasibility of using ChatGPT or a similar AI-based chatbot for patient-provider communication. Design: Survey in January 2023 Setting: Survey Participants: A US representative sample of 400 study participants aged 18 and above was recruited on Prolific, a crowdsourcing platform for academic studies. 384 participants filled out the full survey. After removing participants who spent less than 3 minutes on the survey, 360 respondents remained. 53.3% of respondents analyzed were women; their average age was 45.4. Exposure(s): Ten representative non-administrative patient-provider interactions were extracted from the EHR. Patients' questions were placed in ChatGPT with a request for the chatbot to respond using approximately the same word count as the human provider's response. In the survey, each patient's question was followed by a provider- or ChatGPT-generated response. Participants were informed that five responses were provider-generated and five were chatbot-generated. Participants were asked, and incentivized financially, to correctly identify the response source. Participants were also asked about their trust in chatbots' functions in patient-provider communication, using a Likert scale of 1-5. Main Outcome(s) and Measure(s): Main outcome: Proportion of responses correctly classified as provider- vs chatbot-generated. Secondary outcomes: Average and standard deviation of responses to trust questions. Results: The correct classification of responses ranged between 23.4% to 86.7% for different questions. On average, chatbot responses were identified correctly 60.0% of the time and provider responses were identified correctly 62.3% of the time. On average, responses toward patients' trust in chatbots' functions were weakly positive (mean Likert score: 3.4), with lower trust as the health-related complexity of the task in questions increased. Conclusions and Relevance: ChatGPT responses to patient questions were close to indistinguishable from provider responses. Laypeople appear to trust the use of chatbots to answer lower risk health questions. It is important to continue studying patient-chatbot interaction as chatbots move from administrative to more clinical roles in healthcare.	medRxiv	[{'authorId': '71575036', 'name': 'O. Nov'}, {'authorId': '153599001', 'name': 'N. Singh'}, {'authorId': '121345049', 'name': 'D. Mann'}]
30	cb29cf52f0f7d2e4324c68690a55b22890f2212d	How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection	,	ArXiv	[{'authorId': '2034015747', 'name': 'Biyang Guo'}, {'authorId': '2149173726', 'name': 'Xin Zhang'}, {'authorId': '2108416148', 'name': 'Ziyuan Wang'}, {'authorId': '2152154941', 'name': 'Minqi Jiang'}, {'authorId': '2147399904', 'name': 'Jinran Nie'}, {'authorId': '2193663564', 'name': 'Yuxuan Ding'}, {'authorId': '2170160137', 'name': 'Jianwei Yue'}, {'authorId': '2107934757', 'name': 'Yupeng Wu'}]
31	cb0538917a1810875250672064b1d923eb37c650	AI Insights into Theoretical Physics and the Swampland Program: A Journey Through the Cosmos with ChatGPT	In this case study, we explore the capabilities and limitations of ChatGPT, a natural language processing model developed by OpenAI. We ﬁnd that it is eﬀective at paraphrasing and explaining concepts in a variety of styles, but not at genuinely connecting concepts. It will provide false information with full conﬁdence and make up statements when necessary. However, its ingenious use of language can be fruitful for identifying analogies and describing visual representations of abstract concepts.	ArXiv	[{'authorId': '2201839306', 'name': 'Kay Lehnert'}]
32	8d43f788a863258dc0a34e05f4a0c9f434be7829	Causal-Discovery Performance of ChatGPT in the context of Neuropathic Pain Diagnosis	ChatGPT has demonstrated exceptional proficiency in natural language conversation, e.g., it can answer a wide range of questions while no previous large language models can. Thus, we would like to push its limit and explore its ability to answer causal discovery questions by using a medical benchmark (Tu et al. 2019) in causal discovery.	ArXiv	[{'authorId': '51132868', 'name': 'Ruibo Tu'}, {'authorId': '144905350', 'name': 'Chao Ma'}, {'authorId': '2200137595', 'name': 'Cheng Zhang'}]
33	18ae3a54f9b89530da0f999ce42d50299eed86c7	Chat2VIS: Generating Data Visualisations via Natural Language using ChatGPT, Codex and GPT-3 Large Language Models	The field of data visualisation has long aimed to generate visualisations from natural language text. Research in Natural Language Interfaces (NLIs) has contributed towards the development of a solution which allow users to interact with data using natural language queries. However, the implementation of NLIs is challenging due to the inherent ambiguity of natural language and frequent underspecification of user queries. This study proposes using large language models (LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directly into visualisations. This paper presents a novel system, Chat2VIS, which leverages LLMs and demonstrates how effective prompt engineering can be conducted in order to extract from LLMs the desired code for visualisations. Chat2VIS shows that the use of pre-trained LLMs together with the proposed priming prompts, offers an accurate and reliable approach to generating visualisations from natural language queries, even those that are highly misspecified and underspecified. This approach demonstrates gains in efficiency and cost reduction in the development of these systems, while attaining greater visualisation inference abilities compared to traditional NLP approaches that use hand-crafted grammar rules. The study compares the performance of two types of GPT-3 models and ChatGPT, while demonstrating that the proposed approach is secure, privacy-preserving, and generalisable to different datasets.		[{'authorId': '2166478496', 'name': 'Paula Maddigan'}, {'authorId': '2656889', 'name': 'Teo Susnjak'}]
34	98609145d54ad258187385d12dd624bddcbd9880	A step-by-step Researcher's Guide to the use of an AI-based transformer in epidemiology: an exploratory analysis of ChatGPT using the STROBE checklist for observational studies	Objectives. This study aims at investigating how early-stage AI-based transformers can support researchers in designing and conducting an epidemiological study. To accomplish this, we used ChatGPT to reformulate the STROBE recommendations into a list of questions to be answered by the transformer itself. We then qualitatively evaluated the coherence and relevance of the transformers outputs. Study design: Descriptive study. Methods. We first chose a study to be used as a basis for the simulation. We then used ChatGPT to transform each STROBE checklist item into specific prompts. Each answer to the respective prompt was evaluated by independent researchers in terms of coherence and relevance. Results. The mean scores assigned to each prompt were heterogeneous. On average, for the coherence domain, the overall mean score was 3.6 out of 5.0, and for relevance it was 3.3 out of 5.0. The lowest scores were assigned to items belonging to the Methods section of the checklist. Conclusions. ChatGPT can be considered as a valuable support for researchers in conducting an epidemiological study, following internationally recognized guidelines and standards. It is crucial for the users to have knowledge on the subject and a critical mindset when evaluating the outputs. The potential benefits of AI in scientific research and publishing are undeniable, but it is crucial to address the risks, and the ethical and legal consequences associated with its use.	medRxiv	[{'authorId': '1992720036', 'name': 'F. Sanmarchi'}, {'authorId': '7433186', 'name': 'D. Golinelli'}, {'authorId': '145216171', 'name': 'A. Bucci'}]
35	2ae3ff4f74e1d58e731573c163ba884dbe2ff697	Regulating ChatGPT and other Large Generative AI Models	Large generative AI models (LGAIMs), such as ChatGPT or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest four strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. In particular, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers. In all areas, regulators and lawmakers need to act fast to keep track with the dynamics of ChatGPT et al.		[{'authorId': '49095886', 'name': 'P. Hacker'}, {'authorId': '2053831555', 'name': 'A. Engel'}, {'authorId': '36216751', 'name': 'M. Mauer'}]
36	c7a1202726442872de207ea62c2223a392afcde2	ChatGPT : Un robot conversationnel peut-il enseigner ?	C’est le phénomène numérique de la fin 2022 : ChatGPT. Cet outil conversationnel qui utilise l’Intelligence Artificielle est encore en phase de test et se présente comme « Optimizing Language Models for Dialogue ». Issu des travaux de OpenAI, il est spectaculaire. Il est même tellement spectaculaire que la maison mère de Google a annoncé un « code rouge » pour ne pas se faire dépasser par ce futur tueur de clics !	Management &amp; Data Science	[{'authorId': '49509218', 'name': 'B. Quinio'}, {'authorId': '2769534', 'name': 'Marc Bidan'}]
37	518ef9a2ec3093a06e879b2395ca448ab90d06d6	Philosophical Questions Concerning the Anthropocene and the Plasticene to ChatGPT and the Moderate Perspectives Derived from Their Responses	ChatGPT, an AI (artificial intelligence) model developed by OpenAI was posed various philosophical questions about the Anthropocene and the Plasticene. The model opined that the naming of the Anthropocene holds significance, that the term Plasticene accurately encapsulates contemporary realities, that the expansion of the concept, such as through the lens of plastic ecology, will foster further study, that plastic rocks are important from the perspective of the Earth’s environment, and that although no one sees the Anthropocene in the traditional sense, future scientists may glean insights into the impacts of humankind’s activities from geological samples. From these moderate responses, we may have the utility to modify our opinions and create opportunities for novel anthropological questions.	Journal of Research in Philosophy and History	[{'authorId': '10721764', 'name': 'S. Furukuma'}]
38	058ea9befdd51159b86783f4378fcddf6a2b6f01	Exploring the Cognitive Dynamics of Artificial Intelligence in the Post-COVID-19 and Learning 3.0 Era: A Case Study of ChatGPT	The emergence of artificial intelligence has incited a paradigm shift across the spectrum of human endeavors, with ChatGPT serving as a catalyst for the transformation of various established domains, including but not limited to education, journalism, security, and ethics. In the post-pandemic era, the widespread adoption of remote work has prompted the educational sector to reassess conventional pedagogical methods. This paper is to scrutinize the underlying psychological principles of ChatGPT, delve into the factors that captivate user attention, and implicate its ramifications on the future of learning. The ultimate objective of this study is to instigate a scholarly discourse on the interplay between technological advancements in education and the evolution of human learning patterns, raising the question of whether technology is driving human evolution or vice versa.		[{'authorId': '2006243724', 'name': 'Lin Luan'}, {'authorId': None, 'name': 'Xi Lin'}, {'authorId': '2204960951', 'name': 'Wenbiao Li'}]
39	35cdc00a4e2bc1f6c253a34a5e3a6f697050d1e9	Evaluating the Performance of ChatGPT in Ophthalmology: An Analysis of its Successes and Shortcomings	We tested the accuracy of ChatGPT, a large language model (LLM), in the ophthalmology question-answering space using two popular multiple choice question banks used for the high-stakes Ophthalmic Knowledge Assessment Program (OKAP) exam. The testing sets were of easy-to-moderate difficulty and were diversified, including recall, interpretation, practical and clinical decision-making problems. ChatGPT achieved 55.8% and 42.7% accuracy in the two 260-question simulated exams. Its performance varied across subspecialties, with the best results in general medicine and the worst in neuro-ophthalmology and ophthalmic pathology and intraocular tumors. These results are encouraging but suggest that specialising LLMs through domain-specific pre-training may be necessary to improve their performance in ophthalmic subspecialties.	medRxiv	[{'authorId': '41132090', 'name': 'F. Antaki'}, {'authorId': '2003766323', 'name': 'S. Touma'}, {'authorId': '1396269411', 'name': 'D. Milad'}, {'authorId': '1412265275', 'name': 'J. El-Khoury'}, {'authorId': '2138280308', 'name': 'R. Duval'}]
40	cd4d112f3f9120d0715f22a9de2ce4720822368c	How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment.	"BACKGROUND
Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing model that can generate conversation-style responses to user input.


OBJECTIVE
This study aimed to evaluate the performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination Step 1 and Step 2 exams, as well as to analyze responses for user interpretability.


METHODS
We used 2 sets of multiple-choice questions to evaluate ChatGPT's performance, each with questions pertaining to Step 1 and Step 2. The first set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The second set was the National Board of Medical Examiners (NBME) free 120 questions. ChatGPT's performance was compared to 2 other large language models, GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated across 3 qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question.


RESULTS
Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT's answer selection was present in 100% of outputs of the NBME data sets. Internal information to the question was present in 96.8% (183/189) of all questions. The presence of information external to the question was 44.5% and 27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P<.001) and NBME-Free-Step2 (P=.001) data sets, respectively.


CONCLUSIONS
ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student. Additionally, we highlight ChatGPT's capacity to provide logic and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as an interactive medical education tool to support learning."	JMIR Medical Education	[{'authorId': '2088656028', 'name': 'Aidan Gilson'}, {'authorId': None, 'name': 'Conrad W Safranek'}, {'authorId': '2204771151', 'name': 'Thomas Huang'}, {'authorId': None, 'name': 'Vimig Socrates'}, {'authorId': '2204872037', 'name': 'Ling Chi'}, {'authorId': '2110665673', 'name': 'R. Taylor'}, {'authorId': '3181480', 'name': 'David Chartash'}]
41	a6b129587f6f16b559d9f3e1742e67b59ffa8423	ChatGPT for Clinical Vignette Generation, Revision, and Evaluation	Objective To determine the capabilities of ChatGPT for rapidly generating, rewriting, and evaluating (via diagnostic and triage accuracy) sets of clinical vignettes. Design We explored the capabilities of ChatGPT for generating and rewriting vignettes. First, we gave it natural language prompts to generate 10 new sets of 10 vignettes, each set for a different common childhood illness. Next, we had it generate 10 sets of 10 vignettes given a set of symptoms from which to draw. We then had it rewrite 15 existing pediatric vignettes at different levels of health literacy. Fourth, we asked it to generate 10 vignettes written as a parent, and rewrite these vignettes as a physician, then at a grade 8 reading level, before rewriting them from the original parent's perspective. Finally, we evaluated ChatGPT for diagnosis and triage for 45 clinical vignettes previously used for evaluating symptom checkers. Setting and participants ChatGPT, a publicly available, free chatbot. Main outcome measures Our main outcomes for de novo vignette generation were whether ChatGPT followed vignette creation instructions consistently, correctly, and listed reasonable symptoms for the disease being described. For generating vignettes from pre-existing symptom sets, we examined whether the symptom sets were used without introducing extra symptoms. Our main outcome for rewriting existing standardized vignettes to match patient demographics, and rewriting vignettes between styles, was whether symptoms were dropped or added outside the original vignette. Finally, our main outcomes examining diagnostic and triage accuracy on 45 standardized patient vignettes were whether the correct diagnosis was listed first, and if the correct triage recommendation was made. Results ChatGPT was able to quickly produce varied contexts and symptom profiles when writing vignettes based on an illness name, but overused some core disease symptoms. It was able to use given symptom lists as the basis for vignettes consistently, adding one additional (though appropriate) symptom from outside the list for one disease. Pediatric vignettes rewritten at different levels of health literacy showed more complex symptoms being dropped when writing at low health literacy in 87.5% of cases. While writing at high health literacy, it added a diagnosis to 80% of vignettes (91.7% correctly diagnosed). Symptoms were retained in 90% of cases when rewriting vignettes between viewpoints. When presented with 45 vignettes, ChatGPT identified illnesses with 75.6% (95% CI, 62.6% to 88.5%) first-pass diagnostic accuracy and 57.8% (95% CI, 42.9% to 72.7%) triage accuracy. Its use does require monitoring and has caveats, which we discuss. Conclusions ChatGPT was capable, with caveats and appropriate review, of generating, rewriting, and evaluating clinical vignettes.	medRxiv	[{'authorId': '2070976921', 'name': 'J. Benoît'}]
42	6deec8aa36b057b25123a540922c7007aaaeba60	ChatGPT: Future Directions and Open possibilities	"ChatGPT, the cutting-edge language model developed by OpenAI, is one of the most exciting advancements in the field
of artificial intelligence. With its ability to generate human-like text and respond to complex questions, ChatGPT has
already made a significant impact and is poised to continue its rapid progression in the coming years. As we look to the
future of ChatGPT and large language models, there are many exciting possibilities and open opportunities for this
technology to enhance our lives and change the way we interact with technology"	Mesopotamian Journal of Cyber Security	[{'authorId': '1447250772', 'name': 'Mohammad Aljanabi'}, {'authorId': '2196932745', 'name': 'chatGPT'}]
43	e619bd3335496021e237ac186023bfe44d47104a	La percepción de la Inteligencia Artificial en contextos educativos tras el lanzamiento de ChatGPT: disrupción o pánico	El año 2022 ha finalizado con una de esas innovaciones tecnológicas que tienen un comportamiento difícil de predecir, un cisne negro, acaparando la atención en los medios de comunicación tradicionales y los medios digitales. Efectivamente, se trata de ChatGPT. Si bien la inteligencia artificial ya venía ocupando un lugar destacado en diversas noticias, aunque muchas veces enmascarada bajo otras diversas acepciones, el fenómeno ChatGPT ha vuelto a poner en primera plana esta disciplina, así como sus efectos, tanto positivos como negativos, en nuestra sociedad. Las reacciones a su lanzamiento, sobre todo influidas por su facilidad de acceso y uso, están siendo de lo más variadas, yendo del entusiasmo de los innovadores y adoptadores tempranos hasta el terror casi apocalíptico propio de la película Terminator. De las múltiples aplicaciones de esta herramienta, el mayor debate está centrándose en sus implicaciones en la Educación y en la Academia, por su tremenda potencia para generar textos que perfectamente podrían pasar por creaciones humanas. Estamos en los albores de una tecnología que ha pasado de ser una herramienta de juguete a presentar su candidatura a convertirse en una innovación disruptiva. Que lo consiga o no dependerá de muchos factores, pero si no es esta será otra similar. Negarlo o prohibirlo no servirá absolutamente de nada para parar el efecto tsunami que ya ha comenzado. Por todo ello, primero hay que entender estas tecnologías basadas en modelos de lenguaje y conocer tanto sus beneficios como sus puntos débiles, además de lo que realmente suponen para un sector de actividad específico, como puede ser la Educación. Después de conocer la tecnología y la herramienta, se estaría en condiciones de utilizar (o no) su potencial y de prevenir o detectar sus posibles efectos perniciosos, seguramente cambiando y adaptando procesos que probablemente se tengan muy arraigados y que, por tanto, obliguen a salir de la zona de confort, lo que siempre es causa de resistencia al cambio y de reacciones extremas que, normalmente, no van a parar el camino de una tecnología hacia su meseta de productividad cuando esta llegue a ser parte cotidiana de una mayoría suficiente de usuarios, máxime cuando además se trata de herramientas transversales que van a contagiar sus patrones de uso entre los diferentes dominios de aplicación.	Education in the Knowledge Society (EKS)	[{'authorId': None, 'name': 'Francisco José García-Peñalvo'}]
44	dfc125ec03e84c2ae26147964fc301b1455f3181	The moral authority of ChatGPT	ChatGPT is not only fun to chat with, but it also searches information, answers questions, and gives advice. With consistent moral advice, it might improve the moral judgment and decisions of users, who often hold contradictory moral beliefs. Unfortunately, ChatGPT turns out highly inconsistent as a moral advisor. Nonetheless, it influences users’ moral judgment, we find in an experiment, even if they know they are advised by a chatting bot, and they underestimate how much they are influenced. Thus, ChatGPT threatens to corrupt rather than improves users’ judgment. These findings raise the question of how to ensure the responsible use of ChatGPT and similar AI. Transparency is often touted but seems ineffective. We propose training to improve digital literacy.	ArXiv	[{'authorId': '104246606', 'name': 'Sebastian Krügel'}, {'authorId': '20658415', 'name': 'Andreas Ostermaier'}, {'authorId': '1696033469', 'name': 'Matthias W. Uhl'}]
45	57be0eee785bbfd669c1f51e9a3681105b7f82be	The Benefits and Challenges of ChatGPT: An Overview	This paper provides an overview of ChatGPT, a natural language processing (NLP) system developed by Open AI. It discusses the features of ChatGPT, its benefits, and its challenges. The paper also provides an analysis of the potential applications of ChatGPT and its limitations. The paper concludes that ChatGPT is a powerful NLP system that can generate human-like conversations, but it has some challenges that must be addressed.	Frontiers in Computing and Intelligent Systems	[{'authorId': '2200923644', 'name': 'Jianyang Deng'}, {'authorId': '2201012875', 'name': 'Yijia Lin'}]
46	d05ba0c40f3408aab7bb594628f24d9f04bf2831	Evaluating ChatGPT as an Adjunct for Radiologic Decision-Making	BACKGROUND ChatGPT, a popular new large language model (LLM) built by OpenAI, has shown impressive performance in a number of specialized applications. Despite the rising popularity and performance of AI, studies evaluating the use of LLMs for clinical decision support are lacking. PURPOSE To evaluate ChatGPT's capacity for clinical decision support in radiology via the identification of appropriate imaging services for two important clinical presentations: breast cancer screening and breast pain. MATERIALS AND METHODS We compared ChatGPT's responses to the American College of Radiology (ACR) Appropriateness Criteria for breast pain and breast cancer screening. Our prompt formats included an open-ended (OE) format, where ChatGPT was asked to provide the single most appropriate imaging procedure, and a select all that apply (SATA) format, where ChatGPT was given a list of imaging modalities to assess. Scoring criteria evaluated whether proposed imaging modalities were in accordance with ACR guidelines. RESULTS ChatGPT achieved an average OE score of 1.83 (out of 2) and a SATA average percentage correct of 88.9% for breast cancer screening prompts, and an average OE score of 1.125 (out of 2) and a SATA average percentage correct of 58.3% for breast pain prompts. CONCLUSION Our results demonstrate the feasibility of using ChatGPT for radiologic decision making, with the potential to improve clinical workflow and responsible use of radiology services.	medRxiv	[{'authorId': '47652637', 'name': 'A. S. Rao'}, {'authorId': '103525980', 'name': 'J. Kim'}, {'authorId': '72458178', 'name': 'M. Kamineni'}, {'authorId': '9690134', 'name': 'M. Pang'}, {'authorId': '102502817', 'name': 'W. Lie'}, {'authorId': '14920559', 'name': 'M. Succi'}]
47	b0313223b5debcdd44dfdaf61275fb4daab51861	A Categorical Archive of ChatGPT Failures	Large language models have been demonstrated to be valuable in different fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of data and simulates human conversation by comprehending context and generating appropriate responses. It has garnered significant attention due to its ability to effectively answer a broad range of human inquiries, with fluent and comprehensive answers surpassing prior public chatbots in both security and usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking, which is the focus of this study. Ten categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. The risks, limitations, and societal implications of ChatGPT are also highlighted. The goal of this study is to assist researchers and developers in enhancing future language models and chatbots.		[{'authorId': '3177797', 'name': 'A. Borji'}]
48	2af327673f929d30da88a4c61e0a5bddda389fcd	Microsoft bietet ChatGPT an	Microsoft will Kunden seines Cloud-Dienstes Azure bald die Chat-Software ChatGPT verfügbar machen. US-Medien hatten zuletzt über einen Milliardendeal berichtet, mit dem sich der Konzern ein Drittel an der ChatGPT-Mutter OpenAI sichern möchte.	Lebensmittel Zeitung	[{'authorId': '2133051672', 'name': 'Maurizio Giuri'}]
49	3b1155c1e3ff393ab1cdfd1268b0a59a509e1d1a	Penggunaan ChatGPT Untuk Pendidikan di Era Education 4.0: Usulan Inovasi Meningkatkan Keterampilan Menulis	ChatGPT OpenAI merupakan teknologi mesin berbasis kecerdasan buatan yang dilatih untuk bisa menirukan percakapan manusia menggunakan teknologi NLP (Natural Language Processing). Pada kenyataannya ChatGPT dapat dimanfaatkan untuk menghasilkan suatu tulisan yang cukup ilmiah atau bahkan buku dengan prompt yang dirumuskan di awal dengan teknik yang baik dan efektif. Sehingga peluang inovasi menggunakan teknologi ini terbuka lebar untuk pendidikan di Indonesia, salah satunya dalam meningkatkan kemampuan menulis peserta didik di sekolah/kampus untuk meraih enam kompetensi yang dibutuhkan di Era Education 4.0. Enam kompetensi itu adalah berpikir kritis, kolaborasi, komunikasi, kreativitas, pendidikan karakter dan kewarganegaraan. Hasil eksperimen yang dilakukan menggunakan ChatGPT dapat menghasilkan suatu tulisan berjumlah 693 kata di mana hasil ini masih bisa dikembangkan lebih lanjut untuk penugasan berikutnya bagi peserta didik. Total waktu yang dibutuhkan untuk menyelesaikan eksperimen ini lebih kurang 7 menit, termasuk waktu untuk mendokumentasikan hasil pemrosesan ChatGPT, namun tidak termasuk waktu untuk merumuskan dua prompt yang baik dan efektif sebelum eksperimen dilakukan.	JURNAL PETISI (Pendidikan Teknologi Informasi)	[{'authorId': None, 'name': 'Adi Setiawan'}, {'authorId': None, 'name': 'Ulfah Khairiyah Luthfiyani'}]
50	4cd3ae84e24cfff89ef022e36991df314aac83e2	Will ChatGPT get you caught? Rethinking of Plagiarism Detection	The rise of Artificial Intelligence (AI) technology and its impact on education has been a topic of growing concern in recent years. The new generation AI systems such as chatbots have become more accessible on the Internet and stronger in terms of capabilities. The use of chatbots, particularly ChatGPT, for generating academic essays at schools and colleges has sparked fears among scholars. This study aims to explore the originality of contents produced by one of the most popular AI chatbots, ChatGPT. To this end, two popular plagiarism detection tools were used to evaluate the originality of 50 essays generated by ChatGPT on various topics. Our results manifest that ChatGPT has a great potential to generate sophisticated text outputs without being well caught by the plagiarism check software. In other words, ChatGPT can create content on many topics with high originality as if they were written by someone. These findings align with the recent concerns about students using chatbots for an easy shortcut to success with minimal or no effort. Moreover, ChatGPT was asked to verify if the essays were generated by itself, as an additional measure of plagiarism check, and it showed superior performance compared to the traditional plagiarism-detection tools. The paper discusses the need for institutions to consider appropriate measures to mitigate potential plagiarism issues and advise on the ongoing debate surrounding the impact of AI technology on education. Further implications are discussed in the paper.		[{'authorId': '88352234', 'name': 'M. Khalil'}, {'authorId': '1940086', 'name': 'Erkan Er'}]
51	58b98ccd256f1ebe8551faa6798cfbfc80c490a5	Investigating the use of ChatGPT for the scheduling of construction projects	Large language models such as ChatGPT have the potential to revolutionize the construction industry by automating repetitive and time-consuming tasks. This paper presents a study in which ChatGPT was used to generate a construction schedule for a simple construction project. The output from ChatGPT was evaluated by a pool of participants that provided feedback regarding their overall interaction experience and the quality of the output. The results show that ChatGPT can generate a coherent schedule that follows a logical approach to fulfill the requirements of the scope indicated. The participants had an overall positive interaction experience and indicated the great potential of such a tool to automate many preliminary and time-consuming tasks. However, the technology still has limitations, and further development is needed before it can be widely adopted in the industry. Overall, this study highlights the potential of using large language models in the construction industry and the need for further research.		[{'authorId': '2204579033', 'name': 'Samuel A. Prieto'}, {'authorId': '2166277457', 'name': 'Eyob Mengiste'}, {'authorId': '98018377', 'name': 'Borja García de Soto'}]
52	807110054f8137a10ed5bb05a33e93bf596f897a	ChatGPT: Exploring the Role of Cybersecurity in the Protection of Medical Information	ChatGPT is a large language model developed by OpenAI. It is trained on a dataset of conversational text and can be used to generate human-like responses to prompts in a variety of languages and formats. It can be used for tasks such as chatbots, language translation, and text completion. The role of ChatGPT is to generate human-like text based on a given prompt or context. It can be used in a variety of applications such as chatbots, language translation, text completion, and question answering. Additionally, it can be fine-tuned for specific tasks such as generating product descriptions or summarizing articles. It can also be used to generate creative writing such as poetry and stories. It can be integrated into a wide range of industries from customer service to entertainment, to research.	Mesopotamian Journal of Cyber Security	[{'authorId': '1394846990', 'name': 'Maad M. Mijwil'}, {'authorId': '1447250772', 'name': 'Mohammad Aljanabi'}, {'authorId': '145907529', 'name': 'Ahmed Ali'}]
53	8927db4ee890bf42608752bb840bc9d7db556da1	ChatGPT and Software Testing Education: Promises&Perils	"Over the past decade, predictive language modeling for code has proven to be a valuable tool for enabling new forms of automation for developers. More recently, we have seen the advent of general purpose""large language models"", based on neural transformer architectures, that have been trained on massive datasets of human written text spanning code and natural language. However, despite the demonstrated representational power of such models, interacting with them has historically been constrained to specific task settings, limiting their general applicability. Many of these limitations were recently overcome with the introduction of ChatGPT, a language model created by OpenAI and trained to operate as a conversational agent, enabling it to answer questions and respond to a wide variety of commands from end-users. The introduction of models, such as ChatGPT, has already spurred fervent discussion from educators, ranging from fear that students could use these AI tools to circumvent learning, to excitement about the new types of learning opportunities that they might unlock. However, given the nascent nature of these tools, we currently lack fundamental knowledge related to how well they perform in different educational settings, and the potential promise (or danger) that they might pose to traditional forms of instruction. As such, in this paper, we examine how well ChatGPT performs when tasked with solving common questions in a popular software testing curriculum. Our findings indicate that ChatGPT can provide correct or partially correct answers in 44% of cases, provide correct or partially correct explanations of answers in 57% of cases, and that prompting the tool in a shared question context leads to a marginally higher rate of correct answers. Based on these findings, we discuss the potential promise, and dangers related to the use of ChatGPT by students and instructors."		[{'authorId': '2204651610', 'name': 'Sajed Jalil'}, {'authorId': '1491244699', 'name': 'Suzzana Rafi'}, {'authorId': '1683595', 'name': 'Thomas D. LaToza'}, {'authorId': '1381376597', 'name': 'Kevin Moran'}, {'authorId': '2204647461', 'name': 'Wing Lam'}]
54	75eb3c34edc91fdc7b895e9532d4556ef2a7c0a6	Pânico na Academia! Inteligência Artificial na Construção de Textos Científicos Com o Uso do ChatGPT	A inteligência artificial (IA) por meio do ChatGPT ganhou os noticiários nas últimas semanas. O meio acadêmico gerou ruido com as consequências negativas do uso dessa ferramenta na construção de artigos científicos, dissertações e teses. Aqui fizemos uso desta ferramenta elaborando um texto sobre o papel do marketing em tempos de ataques a democracia no Brasil. O ChatGPT faz com que questões éticas, de inovação e autoria sejam questionadas. Como será o futuro da escrita acadêmica. Estamos diante de um aliado ou de um inimigo?	Revista Interdisciplinar de Marketing	[{'authorId': None, 'name': 'Salomão Alencar de Farias'}]
55	df239785e6d26a45e9c8e06551cfecba92d1ecad	Exploring AI Ethics of ChatGPT: A Diagnostic Analysis	—Recent breakthroughs in natural language process- ing (NLP) have permitted the synthesis and comprehension of coherent text in an open-ended way, therefore translating the theoretical algorithms into practical applications. The large language-model (LLM) has signiﬁcantly impacted businesses such as report summarization softwares and copywriters. Observa-tions indicate, however, that LLMs may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility. Large-scale benchmarks for ac- countable LLMs should consequently be developed. Although several empirical investigations reveal the existence of a few ethical difﬁculties in advanced LLMs, there is no systematic examination and user study of the ethics of current LLMs use. To further educate future efforts on constructing ethical LLMs responsibly, we perform a qualitative research method on OpenAI’s ChatGPT 1 to better understand the practical features of ethical dangers in recent LLMs. We analyze ChatGPT comprehensively from four perspectives: 1) Bias 2) Reliability 3) Robustness 4) Toxicity . In accordance with our stated viewpoints, we empirically benchmark ChatGPT on multiple sample datasets. We ﬁnd that a signiﬁcant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies. In addition, we examine the implications of our ﬁndings on the AI ethics of ChatGPT, as well as future problems and practical design considerations for LLMs. We believe that our ﬁndings may give light on future efforts to determine and mitigate the ethical hazards posed by machines in LLM applications.	ArXiv	[{'authorId': '2080123731', 'name': 'Terry Yue Zhuo'}, {'authorId': '2108733121', 'name': 'Yujin Huang'}, {'authorId': '46729152', 'name': 'Chunyang Chen'}, {'authorId': '3138980', 'name': 'Zhenchang Xing'}]
56	29a32648fe0cb09980697c28176113439a81a64d	ChatGPT - Reshaping medical education and clinical management	"Artificial Intelligence is no more the talk of the fiction read in novels or seen in movies. It has been making inroads slowly and gradually in medical education and clinical management of patients apart from all other walks of life. Recently, chatbots particularly ChatGPT, were developed and trained, using a huge amount of textual data from the internet. This has made a significant impact on our approach in medical science. Though there are benefits of this new technology, a lot of caution is required for its use. 
doi: https://doi.org/10.12669/pjms.39.2.7653 
How to cite this: Khan RA, Jawaid M, Khan AR, Sajjad M. ChatGPT - Reshaping medical education and clinical management. Pak J Med Sci. 2023;39(2):---------. doi: https://doi.org/10.12669/pjms.39.2.7653 
This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited."	Pakistan Journal of Medical Sciences	[{'authorId': '38252945', 'name': 'R. Khan'}, {'authorId': None, 'name': 'Masood Jawaid'}, {'authorId': '2204962760', 'name': 'Aymen Rehan Khan'}, {'authorId': None, 'name': 'Madiha Sajjad'}]
57	b6f8cffc5da51581aec71d919d010d55e5ac068a	The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation	Conversational artificial intelligence (AI) disrupts how humans interact with technology. Recently, OpenAI introduced ChatGPT, a state-of-the-art dialogue model that can converse with its human counterparts with unprecedented capabilities. ChatGPT has witnessed tremendous attention from the media, academia, industry, and the general public, attracting more than a million users within days of its release. However, its explosive adoption for information search and as an automated decision aid underscores the importance to understand its limitations and biases. This paper focuses on one of democratic soci-ety’s most important decision-making processes: political elections. Prompting ChatGPT with 630 political statements from two leading voting advice applications and the nation-agnostic political compass test in three pre-registered experiments, we uncover ChatGPT’s pro-environmental, left-libertarian ideology. For example, ChatGPT would impose taxes on flights, re-strict rent increases, and legalize abortion. In the 2021 elections, it would have voted most likely for the Greens both in Germany (Bündnis 90/Die Grünen) and in the Netherlands (Groen-Links). Our findings are robust when negating the prompts, reversing the order of the statements, varying prompt formality, and across languages (English, German, Dutch, and Spanish). We conclude by discussing the implications of politically biased conversational AI on society.	SSRN Electronic Journal	[{'authorId': '39697129', 'name': 'Jochen Hartmann'}, {'authorId': '2004368787', 'name': 'Jasper Schwenzow'}, {'authorId': '94737364', 'name': 'Maximilian Witte'}]
58	4ec53531cc7c4cde9437dda51ffb173367bb7e25	Assessing the performance of ChatGPT in answering questions regarding cirrhosis and hepatocellular carcinoma	Background: Patients with cirrhosis and hepatocellular carcinoma (HCC) require extensive care. Personalized education can improve their outcomes. ChatGPT (Generative Pre-trained Transformer), a natural language processing model, has shown potential to provide professional yet patient-freindly responses. Aim: To examine the accuracy and reproducibility of ChatGPT in responding to questions regarding knowledge, management, and emotional support for cirrhosis and HCC. Method: ChatGPT's responses to 164 frequently asked questions were independently graded by two transplant hepatologists, with a third reviewer resolving any discrepancies. We also compared the performance of ChatGPT on two previously validated and published questionnaires to the physicians or trainees who were tested in the included publications. Furthermore, we formulated the 26 quality measures of cirrhosis management into questions and tested ChatGPT's knowledge in cirrhosis care. Finally, the capacity to provide emotional support to patients or caregivers was tested. Results: ChatGPT regurgitated extensive knowledge about both cirrhosis and HCC, but for questions with correct responses, only a small proportion was labelled as comprehensive. The performance was better in basic knowledge, lifestyle, and treatment than in the domains of diagnosis and preventive medicine. For the quality measures, the model answered 76.9% of questions correctly but failed to specify the cut-off values for making medical decisions and treatment durations. When compared to physicians/trainees, ChatGPT fell short in knowledge of guidelines varying across geographic regions, such as HCC screening criteria. The model also provided practical and multifaceted advice to patients and caregivers regarding the next steps and adjusting to a new diagnosis. Conclusion: In summary, we analyzed the areas of robustness and limitations of ChatGPT's responses on the management of cirrhosis and HCC and relevant emotional support. ChatGPT may have a role as an adjunct informational tool for patients and physicians to improve outcomes.	medRxiv	[{'authorId': '6807774', 'name': 'Y. Yeo'}, {'authorId': '84723205', 'name': 'J. Samaan'}, {'authorId': '30987139', 'name': 'W. Ng'}, {'authorId': '47388850', 'name': 'Peng-Sheng Ting'}, {'authorId': '2059999783', 'name': 'H. Trivedi'}, {'authorId': '14685491', 'name': 'A. Vipani'}, {'authorId': '6749804', 'name': 'W. Ayoub'}, {'authorId': '9125596', 'name': 'J. D. Yang'}, {'authorId': '14798289', 'name': 'O. Liran'}, {'authorId': '144729350', 'name': 'B. Spiegel'}, {'authorId': '2184361653', 'name': 'A. Kuo'}]
59	ffa13df31597b8952af56a2a1bea719726422724	ChatGpt: Open Possibilities		Iraqi Journal for Computer Science and Mathematics	[]
60	9460f9fbc532416c8ec39591577032a5fa4a79b3	ChatGPT: Bullshit spewer or the end of traditional assessments in higher education?		1	[]
61	372fe9428dff788274081b00634d484db0c2fda4	ChatGPT Goes to Law School		SSRN Electronic Journal	[{'authorId': '46308962', 'name': 'Jonathan H. Choi'}, {'authorId': '98900189', 'name': 'Kristin E. Hickman'}, {'authorId': '16166682', 'name': 'Amy B. Monahan'}, {'authorId': '83686718', 'name': 'D. Schwarcz'}]
62	283ecf85ba901865303f37a2c9f6206d266e5019	ChatGPT: evolution or revolution?		Medicine, Health care and Philosophy	[{'authorId': '2354377', 'name': 'B. Gordijn'}, {'authorId': '50377385', 'name': 'H. Have'}]
63	29243fe762ff627bff2d4f34f44cfcaa0afce3fd	ChatGPT and the Future of Medical Writing.		Radiology	[{'authorId': '2150472791', 'name': 'S. Biswas'}]
64	b465f197feb976140b40fe059101c8d5b4439645	Daily briefing: ChatGPT listed as author on research papers.		Nature	[{'authorId': '82335473', 'name': 'Flora Graham'}]
65	a99df4187e94a842504759eaa61ce864a31f30a3	A Computer Wrote this Paper: What ChatGPT Means for Education, Research, and Writing		SSRN Electronic Journal	[{'authorId': '120912545', 'name': 'Leah M. Bishop'}]
66	2db2ccb54f68ab20edc5ae94eceb1cc16359e142	OpenAI ChatGPT Generated Results: Similarity Index of Artificial Intelligence-Based Contents		SSRN Electronic Journal	[{'authorId': '72433942', 'name': 'R. J. Ventayen'}]
67	ea8c37b949545aa600d1f9410c0040098848dbca	A conversation with ChatGPT on the role of computational systems biology in stem cell research		Stem Cell Reports	[{'authorId': '3284810', 'name': 'P. Cahan'}, {'authorId': '5746516', 'name': 'B. Treutlein'}]
68	621b8c4b37d1e22d440262dda2206ac13da6d779	ChatGPT and Other Large Language Models Are Double-edged Swords.		Radiology	[{'authorId': '35139543', 'name': 'Yiqiu Shen'}, {'authorId': '3771591', 'name': 'L. Heacock'}, {'authorId': '2202899600', 'name': 'Jonathan Elias'}, {'authorId': '4964392', 'name': 'K. Hentel'}, {'authorId': '12542421', 'name': 'B. Reig'}, {'authorId': '2202899450', 'name': 'George Shih'}, {'authorId': '145717702', 'name': 'Linda Moy'}]
69	c141baa361ae64d45764142f8eb7cdff14991933	Issues in the 3rd year of the COVID-19 pandemic, including computer-based testing, study design, ChatGPT, journal metrics, and appreciation to reviewers.		Journal of Educational Evaluation for Health Professions	[{'authorId': '143918822', 'name': 'Sun Huh'}]
70	aab413673beaa1e2f1176351356c40be8ffef9e2	To ChatGPT or not to ChatGPT? The Impact of Artificial Intelligence on Academic Publishing.		The Pediatric Infectious Disease Journal	[{'authorId': '143739014', 'name': 'N. Curtis'}]
71	d91bd7bdea31775302a8a0b997b6d67bf20ac297	Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?	Systematic reviews are comprehensive reviews of the literature for a highly focused research question. These reviews are often treated as the highest form of evidence in evidence-based medicine, and are the key strategy to answer research questions in the medical field. To create a high-quality systematic review, complex Boolean queries are often constructed to retrieve studies for the review topic. However, it often takes a long time for systematic review researchers to construct a high quality systematic review Boolean query, and often the resulting queries are far from effective. Poor queries may lead to biased or invalid reviews, because they missed to retrieve key evidence, or to extensive increase in review costs, because they retrieved too many irrelevant studies. Recent advances in Transformer-based generative models have shown great potential to effectively follow instructions from users and generate answers based on the instructions being made. In this paper, we investigate the effectiveness of the latest of such models, ChatGPT, in generating effective Boolean queries for systematic review literature search. Through a number of extensive experiments on standard test collections for the task, we find that ChatGPT is capable of generating queries that lead to high search precision, although trading-off this for recall. Overall, our study demonstrates the potential of ChatGPT in generating effective Boolean queries for systematic review literature search. The ability of ChatGPT to follow complex instructions and generate queries with high precision makes it a valuable tool for researchers conducting systematic reviews, particularly for rapid reviews where time is a constraint and often trading-off higher precision for lower recall is acceptable.		[{'authorId': '2146514461', 'name': 'Shuai Wang'}, {'authorId': '8842143', 'name': 'Harrisen Scells'}, {'authorId': '1783566', 'name': 'B. Koopman'}, {'authorId': '1692855', 'name': 'G. Zuccon'}]
72	afcb7782c7da4e2d8731e646f3cec4aee8e13a94	This Isn't Another Piece on ChatGPT		The National Teaching &amp; Learning Forum	[{'authorId': '50984179', 'name': 'L. Bessette'}]
73	6ea22dc509377c076abca0d668ff6404661f4bce	ChatGPT & Generative AI Systems as Quasi-Expert Legal Advice Lawyers - Case Study Considering Potential Appeal Against Conviction of Tom Hayes		SSRN Electronic Journal	[{'authorId': '1423468758', 'name': 'Rupert Macey-Dare'}]
74	578c39d6bcf1a9b4167e66aaa2140cc7ddf068ad	Chatting With ChatGPT		ChemViews	[{'authorId': '2075937681', 'name': 'Vera Koester'}, {'authorId': '12374747', 'name': 'Catharina Goedecke'}]
75	eb21b2d1e82c7ec6f78f43e5eb4692456bf3e21e	ChatGPT: five priorities for research		Nature	[{'authorId': '51140547', 'name': 'E. A. V. van Dis'}, {'authorId': '119315380', 'name': 'J. Bollen'}, {'authorId': '2067166918', 'name': 'W. Zuidema'}, {'authorId': '8457319', 'name': 'R. van Rooij'}, {'authorId': '38196528', 'name': 'C. Bockting'}]
76	819f91874cfa43335401cddedd02b8cfc1c2b84f	Mathematical Capabilities of ChatGPT	,	ArXiv	[{'authorId': '2127069744', 'name': 'Simon Frieder'}, {'authorId': '2151791007', 'name': 'Luca Pinchetti'}, {'authorId': '117074363', 'name': 'Ryan-Rhys Griffiths'}, {'authorId': '148237421', 'name': 'Tommaso Salvatori'}, {'authorId': '1690572', 'name': 'Thomas Lukasiewicz'}, {'authorId': '2203427985', 'name': 'Philipp Christian Petersen'}, {'authorId': '2203426491', 'name': 'Alexis Chevalier'}, {'authorId': '20611061', 'name': 'J. Berner'}]
77	4b16838eace10cad525641b3568ecedebe12495b	Chatgpt's Scientific Writings: A Case Study on Traffic Safety		SSRN Electronic Journal	[{'authorId': '14936984', 'name': 'Boniphace Kutela'}, {'authorId': '2204047232', 'name': 'Kelvin Msechu'}, {'authorId': '2109619283', 'name': 'Subasish Das'}, {'authorId': '51057895', 'name': 'E. Kidando'}]
78	669e09e5a911e2ea70ade1962f39e3b276724fde	ChatGPT can find and fix the bugs in computer code		New Scientist	[{'authorId': '1413934873', 'name': 'Chris Stokel-Walker'}]
79	55c75a0d583f847b3bc528d61d2da6b0aca1427f	Discursive Competence in ChatGPT, Part 1: Talking with Dragons		SSRN Electronic Journal	[{'authorId': '3195738', 'name': 'W. Benzon'}]
80	494575126d019ec221ad3a03970c4a00b6b45489	Can ChatGPT 'Think Like a Lawyer?' A Socratic Dialogue		SSRN Electronic Journal	[{'authorId': '120912545', 'name': 'Leah M. Bishop'}]
81	d32adaaf927c7fcfcde42e5add984c39b2bb8bd6	Daily briefing: Science urgently needs a plan for ChatGPT.		Nature	[{'authorId': '82335473', 'name': 'Flora Graham'}]
82	c4d96dfef7b67ea371354b416d1e458aa65f8177	Is ChatGPT Leading Generative AI? What is Beyond Expectations?		SSRN Electronic Journal	[{'authorId': '30528739', 'name': 'Ömer Aydın'}, {'authorId': '40631192', 'name': 'Enis Karaarslan'}]
83	0c51893d25117c39a9e84c1e021ce4a23970f011	The Usefulness and Challenges of Chatbots for Accounting Professionals: Application On ChatGPT		SSRN Electronic Journal	[{'authorId': '120966380', 'name': 'Hashem Alshurafat'}]
84	5f996fd3592478b6923590ed5347d6206357e95a	What ChatGPT and generative AI mean for science		Nature	[{'authorId': '1413934873', 'name': 'Chris Stokel-Walker'}, {'authorId': '46619889', 'name': 'Richard van Noorden'}]
85	e5a372a58a6118a58c3c429d094c81e94a90992c	ChatGPT, An Artificial Intelligence Chatbot, Is Impacting Medical Literature		Arthroscopy: The Journal of Arthroscopy And Related	[{'authorId': '2175437221', 'name': 'James H. Lubowitz'}]
86	d9a03504f8c5c61e57db8b294e64d7b8eacc5879	Acute Pulmonary Edema After Hyperbaric Oxygen Treatment: A Case Report Written With ChatGPT Assistance		Cureus	[{'authorId': None, 'name': 'Haris M Akhter'}, {'authorId': '47511413', 'name': 'J. Cooper'}]
87	c216e56e1a01c521866bcf07d9761258c42f3e46	Generating scholarly content with ChatGPT: ethical challenges for medical publishing.		The Lancet Digital Health	[{'authorId': '3622189', 'name': 'M. Liebrenz'}, {'authorId': '48348811', 'name': 'R. Schleifer'}, {'authorId': '3537371', 'name': 'A. Buadze'}, {'authorId': '3578748', 'name': 'D. Bhugra'}, {'authorId': '2116709192', 'name': 'Alexander Smith'}]
88	7b6a8c6d44e0f77bf930484e438d77b7465a69fb	Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning		SSRN Electronic Journal	[{'authorId': '1411783122', 'name': 'David Baidoo-Anu'}, {'authorId': '2203232571', 'name': 'Leticia Owusu Ansah'}]
89	86525cf1961a876cbca92c3dbb228da8201595fa	Democratizing Financial Knowledge with ChatGPT by OpenAI: Unleashing the Power of Technology		SSRN Electronic Journal	[{'authorId': '2204312702', 'name': 'Thomas Yue'}, {'authorId': '2187372354', 'name': 'David Au'}, {'authorId': '2204329476', 'name': 'Chi Chung Au'}, {'authorId': '92809033', 'name': 'Kwansai Iu'}]
90	f27dd152582b9da3c3a879025eceaebbf2a50752	Chatting about ChatGPT: How May AI and GPT Impact Academia and Libraries?		SSRN Electronic Journal	[{'authorId': '66673215', 'name': 'B. Lund'}, {'authorId': '2203795113', 'name': 'Wang Ting'}]
91	9ed2cc3ae5ceb9f67b27ebc00ee3fc7d54bcc35d	De Gutenberg à ChatGPT: Le défi de l'université numérique	Le but de ce texte est de mieux cerner l’ampleur du défi que pose le monde numérique au milieu universitaire et de proposer quelques idées pouvant alimenter la réflexion des universitaires dans cette démarche d’adaptation au monde numérique. L’impact de la révolution numérique sur le monde de l’éducation universitaire peut se résumer à ceci : dans un monde numérique, les connaissances sont accessibles à tous et, somme toute à très faibles coûts alors que le niveau et les types de compétences requises pour évoluer dans un monde numérique sont plus complexes. L’université a perdu son quasi-monopole dans la transmission des connaissances et elle n’a pas encore établi pleinement son rôle, pourtant indispensable, dans l’acquisition des nouvelles compétences. L’université est également menacée dans sa capacité historique d’attirer, de retenir et de promouvoir les artisans du monde de demain qui sont de plus en plus actifs dans les écosystèmes animés et même souvent contrôlés par les grands gagnants industriels de la révolution numérique. Une réflexion et une discussion s’imposent. Le monde numérique étant un monde de données, d’information et de savoir, ce monde ne peut être que le monde naturel de la communauté des professeurs, des maîtres, des chercheurs et des étudiants. La numérisation de l’éducation est une chance unique pour le rendre plus accessible au plus grand nombre.		[{'authorId': '114543655', 'name': 'Henri-Paul Rousseau'}]
92	51a7f466057e94b81f8d1b3e15c86586f77f4f2b	Is ChatGPT a Good Tool for T&CM Students in Studying Pharmacology?		SSRN Electronic Journal	[{'authorId': '71936868', 'name': 'Saima Nisar'}, {'authorId': '2057319891', 'name': 'Muhammad Shahzad Aslam'}]
93	024fee7fa8037178e4ead71baa41304e85512bfe	ChatGPT Intimates a Tantalizing Future; Its core LLM is Organized on Multiple Levels; and it has Broken the Idea of Thinking, Version 2		SSRN Electronic Journal	[{'authorId': '3195738', 'name': 'W. Benzon'}]
94	20da6ad1068a8a609daa5ab16af3afc62e5f2b89	ChatGPT Is Shaping the Future of Medical Writing but Still Requires Human Judgment.		Radiology	[{'authorId': '2091400886', 'name': 'F. Kitamura'}]
95	1f44ed0ca7f292837b80a3d380d4fa6abfc4821f	The AI Race is on! Google's Bard and Openai's Chatgpt Head to Head: An Opinion Article		SSRN Electronic Journal	[{'authorId': '119376922', 'name': 'Md. Saidur Rahaman'}, {'authorId': None, 'name': 'M. M. Tahmid Ahsan'}, {'authorId': None, 'name': 'Nishath Anjum'}, {'authorId': None, 'name': 'Md. Mizanur Rahman'}, {'authorId': None, 'name': 'Md Nafizur Rahman'}]
96	d410f915eeb9c92a2970568338804261f1377331	ChatGPT or Human? Detect and Explain. Explaining Decisions of Machine Learning Model for Detecting Short ChatGPT-generated Text	a machine learning	ArXiv	"[{'authorId': '2203427954', 'name': ""Sandra Mitrovi'c""}, {'authorId': '7469234', 'name': 'Davide Andreoletti'}, {'authorId': '40120784', 'name': 'Omran Ayoub'}]"
97	81e8396f2257952819dabd425fd6ac7c48d16fb8	Towards Artificial Intelligence-Based Cybersecurity: The Practices and ChatGPT Generated Ways to Combat Cybercrime	Today, cybersecurity is considered one of the most noteworthy topics that are circulated frequently among companies in order to protect their data from hacking operations. The emergence of cyberspace contributed to the growth of electronic systems. It is a virtual digital space through which interconnection is established between computers and smartphones connected within the Internet of Things environment. This space is critical in building a safe digital environment free of threats and cybercrime. It is only possible to make a digital environment with the presence of cyberspace, which contains modern technologies that make this environment safe and far from unauthorized individuals. Cybersecurity has a wide range of challenges and obstacles in performance, and it is difficult for companies to face them. In this report, the most significant practices, sound, and good strategies will be studied to stop cybercrime and make a digital environment that guarantees data transfers between electronic devices safely and without the presence of malicious software. This report concluded that the procedures provided by cybersecurity are required and must be taken care of and developed.	Iraqi Journal for Computer Science and Mathematics	[{'authorId': '1394846990', 'name': 'Maad M. Mijwil'}]
98	68752232bfc7036d76d23bb5be28b45f02a84bfd	I Asked a ChatGPT to Write an Editorial About How We Can Incorporate Chatbots Into Neurosurgical Research and Patient Care….		Neurosurgery	"[{'authorId': None, 'name': ""Randy S D'Amico""}, {'authorId': '16427284', 'name': 'T. White'}, {'authorId': '2167140767', 'name': 'Harshal A. Shah'}, {'authorId': None, 'name': 'David J Langer'}]"
99	d8459994ffc42e6ac021e47cfa840e144d42e354	Information Literacy, Data Literacy, Privacy Literacy, and ChatGPT: Technology Literacies Align with Perspectives on Emerging Technology Adoption within Communities		SSRN Electronic Journal	[{'authorId': '2000639769', 'name': 'Brady D. Lund'}, {'authorId': '75067375', 'name': 'Daniel A. Agbaji'}]
100	0d065e8688c38bb0148203a1738f47184a5b58d3	ChatGPT: Unlocking the Future of NLP in Finance		SSRN Electronic Journal	[{'authorId': '104406084', 'name': 'Adam Zaremba'}, {'authorId': '16775568', 'name': 'Ender Demir'}]
101	0995fc83865e03a592c21bd5f21ef15f9b3012d1	ChatGPT by OpenAI: The End of Litigation Lawyers?		SSRN Electronic Journal	[{'authorId': '92809033', 'name': 'Kwansai Iu'}, {'authorId': '2190336752', 'name': 'Vanessa Man-Yi Wong'}]
102	a147a3c5eb5ccc6aebc0e8e11f7900c03e5c9b7c	ChatGPT at Universities – The Least of Our Concerns		SSRN Electronic Journal	[{'authorId': '14786067', 'name': 'Jurgen Willems'}]
103	955471dbc9cb4a407351c287a6104daa58779e27	ChatGPT: friend or foe?		The Lancet Digital Health	[{'authorId': '1666213350', 'name': 'The Lancet Digital Health'}]
104	688786138c179a7cb3e9565dd99f7630320e6beb	Who’s Afraid of ChatGPT? An Examination of ChatGPT’s Implications for Legal Writing		SSRN Electronic Journal	[{'authorId': '2174089984', 'name': 'Ashley B. Armstrong'}]
105	1cc81c47ca2932c178747677f331621cf1aa44a8	ChatGPT, Professor of Law		SSRN Electronic Journal	[{'authorId': '2204929112', 'name': 'Tammy Pettinato Oltz'}]
106	60f78afe2040f33988c71d585c3f42f06814d0de	ChatGPT: the future of discharge summaries?		The Lancet Digital Health	[{'authorId': '2125627924', 'name': 'Sajan Patel'}, {'authorId': '2067721105', 'name': 'Kyle Lam'}]
107	024fee7fa8037178e4ead71baa41304e85512bfe	ChatGPT Intimates a Tantalizing Future; Its core LLM is Organized on Multiple Levels; and it has Broken the Idea of Thinking, Version 2		SSRN Electronic Journal	[{'authorId': '3195738', 'name': 'W. Benzon'}]
108	eb21b2d1e82c7ec6f78f43e5eb4692456bf3e21e	ChatGPT: five priorities for research		Nature	[{'authorId': '51140547', 'name': 'E. A. V. van Dis'}, {'authorId': '119315380', 'name': 'J. Bollen'}, {'authorId': '2067166918', 'name': 'W. Zuidema'}, {'authorId': '8457319', 'name': 'R. van Rooij'}, {'authorId': '38196528', 'name': 'C. Bockting'}]
109	d9a03504f8c5c61e57db8b294e64d7b8eacc5879	Acute Pulmonary Edema After Hyperbaric Oxygen Treatment: A Case Report Written With ChatGPT Assistance		Cureus	[{'authorId': None, 'name': 'Haris M Akhter'}, {'authorId': '47511413', 'name': 'J. Cooper'}]
110	669e09e5a911e2ea70ade1962f39e3b276724fde	ChatGPT can find and fix the bugs in computer code		New Scientist	[{'authorId': '1413934873', 'name': 'Chris Stokel-Walker'}]
111	c216e56e1a01c521866bcf07d9761258c42f3e46	Generating scholarly content with ChatGPT: ethical challenges for medical publishing.		The Lancet Digital Health	[{'authorId': '3622189', 'name': 'M. Liebrenz'}, {'authorId': '48348811', 'name': 'R. Schleifer'}, {'authorId': '3537371', 'name': 'A. Buadze'}, {'authorId': '3578748', 'name': 'D. Bhugra'}, {'authorId': '2116709192', 'name': 'Alexander Smith'}]
112	55c75a0d583f847b3bc528d61d2da6b0aca1427f	Discursive Competence in ChatGPT, Part 1: Talking with Dragons		SSRN Electronic Journal	[{'authorId': '3195738', 'name': 'W. Benzon'}]
113	aab413673beaa1e2f1176351356c40be8ffef9e2	To ChatGPT or not to ChatGPT? The Impact of Artificial Intelligence on Academic Publishing.		The Pediatric Infectious Disease Journal	[{'authorId': '143739014', 'name': 'N. Curtis'}]
114	d410f915eeb9c92a2970568338804261f1377331	ChatGPT or Human? Detect and Explain. Explaining Decisions of Machine Learning Model for Detecting Short ChatGPT-generated Text	a machine learning	ArXiv	"[{'authorId': '2203427954', 'name': ""Sandra Mitrovi'c""}, {'authorId': '7469234', 'name': 'Davide Andreoletti'}, {'authorId': '40120784', 'name': 'Omran Ayoub'}]"
115	0ff24246377b740fcb54821384988b090785d6ee	Sixty seconds on . . . ChatGPT		BMJ	[{'authorId': '1620851794', 'name': 'Mun-Keat Looi'}]
116	578c39d6bcf1a9b4167e66aaa2140cc7ddf068ad	Chatting With ChatGPT		ChemViews	[{'authorId': '2075937681', 'name': 'Vera Koester'}, {'authorId': '12374747', 'name': 'Catharina Goedecke'}]
117	819f91874cfa43335401cddedd02b8cfc1c2b84f	Mathematical Capabilities of ChatGPT	,	ArXiv	[{'authorId': '2127069744', 'name': 'Simon Frieder'}, {'authorId': '2151791007', 'name': 'Luca Pinchetti'}, {'authorId': '117074363', 'name': 'Ryan-Rhys Griffiths'}, {'authorId': '148237421', 'name': 'Tommaso Salvatori'}, {'authorId': '1690572', 'name': 'Thomas Lukasiewicz'}, {'authorId': '2203427985', 'name': 'Philipp Christian Petersen'}, {'authorId': '2203426491', 'name': 'Alexis Chevalier'}, {'authorId': '20611061', 'name': 'J. Berner'}]
118	afcb7782c7da4e2d8731e646f3cec4aee8e13a94	This Isn't Another Piece on ChatGPT		The National Teaching &amp; Learning Forum	[{'authorId': '50984179', 'name': 'L. Bessette'}]
119	8c74508785111193ae5171b83cce419e539e0c42	The AI writing on the wall		Nature Machine Intelligence	[]
120	c339ab1edf45f8e789c550907d9fece8ca4a3d24	Welcome to the AI future?		Nature Astronomy	[]
121	c0f3072aeab373fd64c98126afe1b3964ed3438d	Chatbots in a Honeypot World	Question-and-answer agents like ChatGPT offer a novel tool for use as a potential honeypot interface in cyber security. By imitating Linux, Mac, and Windows terminal commands and providing an interface for TeamViewer, nmap, and ping, it is possible to create a dynamic environment that can adapt to the actions of attackers and provide insight into their tactics, techniques, and procedures (TTPs). The paper illustrates ten diverse tasks that a conversational agent or large language model might answer appropriately to the effects of command-line attacker. The original result features feasibility studies for ten model tasks meant for defensive teams to mimic expected honeypot interfaces with minimal risks. Ultimately, the usefulness outside of forensic activities stems from whether the dynamic honeypot can extend the time-to-conquer or otherwise delay attacker timelines short of reaching key network assets like databases or confidential information. While ongoing maintenance and monitoring may be required, ChatGPT's ability to detect and deflect malicious activity makes it a valuable option for organizations seeking to enhance their cyber security posture. Future work will focus on cybersecurity layers, including perimeter security, host virus detection, and data security.	ArXiv	[{'authorId': '2197525782', 'name': 'Forrest McKee'}, {'authorId': '46787948', 'name': 'David Noever'}]
122	9f6655a6e93d9be261a93893fb8469ac8fa4a99d	Rapamycin in the context of Pascal’s Wager: generative pre-trained transformer perspective	Large language models utilizing transformer neural networks and other deep learning architectures demonstrated unprecedented results in many tasks previously accessible only to human intelligence. In this article, we collaborate with ChatGPT, an AI model developed by OpenAI to speculate on the applications of Rapamycin, in the context of Pascal’s Wager philosophical argument commonly utilized to justify the belief in god. In response to the query “Write an exhaustive research perspective on why taking Rapamycin may be more beneficial than not taking Rapamycin from the perspective of Pascal’s wager” ChatGPT provided the pros and cons for the use of Rapamycin considering the preclinical evidence of potential life extension in animals. This article demonstrates the potential of ChatGPT to produce complex philosophical arguments and should not be used for any off-label use of Rapamycin.	Oncoscience	[{'authorId': '2079798046', 'name': 'A. Zhavoronkov'}]
123	ed15d0f9a8eb3728931e772822314efde340e0b1	The Future of AI in Medicine: A Perspective from a Chatbot		Annals of Biomedical Engineering	[{'authorId': '152378504', 'name': 'Michael R King'}]
124	3e5698bcb675eceaa8bc462a38a6a6ea91ddfb1a	The Death of the Short-Form Physics Essay in the Coming AI Revolution	. The latest AI language modules can produce original, high quality full short-form (300-word) Physics essays within seconds. These technologies such as ChatGPT and davinci-003 are freely available to anyone with an internet connection. In this work, we present evidence of AI generated short-form essays achieving ﬁrst-class grades on an essay writing assessment from an accredited, current university Physics module. The assessment requires students answer ﬁve open-ended questions with a short, 300-word essay each. Fifty AI answers were generated to create ten submissions that were independently marked by ﬁve separate markers. The AI generated submissions achieved an average mark of 71 ± 2%, in strong agreement with the current module average of 71 ± 5%. A typical AI submission would therefore most-likely be awarded a First Class, the highest classiﬁcation available at UK universities. Plagiarism detection software returned a plagiarism score between 2 ± 1% (Grammarly) and 7 ± 2% (TurnitIn). We argue that these results indicate that current AI MLPs represent a signiﬁcant threat to the ﬁdelity of short-form essays as an assessment method in Physics courses.		[{'authorId': '3982687', 'name': 'W. Yeadon'}, {'authorId': '2197779028', 'name': 'Oto-Obong Inyang'}, {'authorId': '9799803', 'name': 'Arin Mizouri'}, {'authorId': '47673900', 'name': 'Alex Peach'}, {'authorId': '50820901', 'name': 'Craig P. Testrow'}]
125	ded781afc580a3fc203af051ad2c180acce207ff	The European AI Liability Directives - Critique of a Half-Hearted Approach and Lessons for the Future	The optimal liability framework for AI systems remains an unsolved problem across the globe. With ChatGPT and other large models taking the technology to the next level, solutions are urgently needed. In a much-anticipated move, the European Commission advanced two proposals outlining the European approach to AI liability in September 2022: a novel AI Liability Directive (AILD) and a revision of the Product Liability Directive (PLD). They constitute the final cornerstone of AI regulation in the EU. Crucially, the liability proposals and the AI Act are inherently intertwined: the latter does not contain any individual rights of affected persons, and the former lack specific, substantive rules on AI development and deployment. Taken together, these acts may well trigger a “Brussels effect” in AI regulation, with significant consequences for the US and other countries. Against this background, this paper makes three novel contributions. First, it examines in detail the Commission proposals and shows that, while making steps in the right direction, they ultimately represent a half-hearted approach: if enacted as foreseen, AI liability in the EU will primarily rest on disclosure of evidence mechanisms and a set of narrowly defined presumptions concerning fault, defectiveness and causality. Hence, second, the article makes suggestions for amendments to the proposed AI liability framework. They are collected in a concise Annex at the end of the paper. I argue, inter alia, that the dichotomy between the fault-based AILD Proposal and the supposedly strict liability PLD Proposal is fictional and should be abandoned; that an EU framework for AI liability should comprise one fully harmonizing regulation instead of two insufficiently coordinated directives; and that the current proposals unjustifiably collapse fundamental distinctions between social and individual risk by equating high-risk AI systems in the AI Act with those under the liability framework. Third, based on an analysis of the key risks AI poses, the final part of the paper maps out a road for the future of AI liability and regulation, in the EU and beyond. More specifically, I make four key proposals. Effective compensation should be ensured by combining truly strict liability for certain high-risk AI systems with general presumptions of defectiveness, fault and causality in cases involving SMEs or non-high-risk AI systems. The paper introduces a novel distinction between illegitimateand legitimate-harm models to delineate strict liability’s scope. Truly strict liability should be reserved for high-risk AI systems that, from a social perspective, should not cause harm (illegitimate-harm models, e.g., autonomous vehicles or medical AI). Models meant to cause some unavoidable harm by ranking and rejecting individuals (legitimate-harm models, e.g., credit scoring or insurance scoring) may only face rebuttable presumptions of defectiveness and causality. General-purpose AI systems should only be subjected to high-risk * Professor Dr. Philipp Hacker, LL.M. (Yale), Chair for Law and Ethics of the Digital Society, European New School of Digital Studies, European University Viadrina. I am grateful for comments by Maximilian Eber, Andreas Engel, Rasmus Rothe, and Sandra Wachter. My team provided excellent research assistance, particularly Sarah Großheim and Marco Mauer. II regulation, including liability for high-risk AI systems, in specific high-risk use cases for which they are deployed. Consumers ought to be liable based on regular fault, in general. Furthermore, innovation and legal certainty should be fostered through a comprehensive regime of safe harbours, defined quantitatively to the best extent possible. Moreover, trustworthy AI remains an important goal for AI regulation. Hence, the liability framework must specifically extend to non-discrimination cases and provide for clear rules concerning explainability (XAI). Finally, awareness for the climate effects of AI, and digital technology more broadly, is rapidly growing in computer science. In diametrical opposition to this shift in discourse and understanding, however, EU legislators thoroughly neglect environmental sustainability in both the AI Act and the proposed liability regime. To counter this, I propose to jump-start sustainable AI regulation via sustainability impact assessments in the AI Act and sustainable design defects in the liability regime. In this way, the law may help spur not only fair AI and XAI, but potentially also sustainable AI (SAI).	SSRN Electronic Journal	[{'authorId': '49095886', 'name': 'P. Hacker'}]
126	3b8ccc7ec80b8775de603e248ac1ca2b919d6b70	Chatbots in a Botnet World	Question-and-answer formats provide a novel experimental platform for investigating cybersecurity questions. Unlike previous chatbots, the latest ChatGPT model from OpenAI supports an advanced understanding of complex coding questions. The research demonstrates thirteen coding tasks that generally qualify as stages in the MITRE ATT&CK framework, ranging from credential access to defense evasion. With varying success, the experimental prompts generate examples of keyloggers, logic bombs, obfuscated worms, and payment-fulfilled ransomware. The empirical results illustrate cases that support the broad gain of functionality, including self-replication and self-modification, evasion, and strategic understanding of complex cybersecurity goals. One surprising feature of ChatGPT as a language-only model centers on its ability to spawn coding approaches that yield images that obfuscate or embed executable programming steps or links.	ArXiv	[{'authorId': '2197525782', 'name': 'Forrest McKee'}, {'authorId': '46787948', 'name': 'David Noever'}]
127	288078127a3078332230442170f6745ed333c700	A Conversation on Artificial Intelligence, Chatbots, and Plagiarism in Higher Education		Cellular and Molecular Bioengineering	[{'authorId': '152378504', 'name': 'Michael R King'}, {'authorId': '2196932745', 'name': 'chatGPT'}]
128	b8b9ddf6cd23dec702f90c5f01c1733616f823df	Open artificial intelligence platforms in nursing education: Tools for academic progress or abuse?		Nurse Education in Practice	"[{'authorId': '1404192752', 'name': ""S. O'Connor""}, {'authorId': '2196932745', 'name': 'chatGPT'}]"
129	f4db4a18080e9e4ebba9ba68bb7d34230c84d0c3	Truth Machines: Synthesizing Veracity in AI Language Models	As AI technologies are rolled out into healthcare, academia, human resources, law, and a multitude of other domains, they become de-facto arbiters of truth. But truth is highly contested, with many different definitions and approaches. This article discusses the struggle for truth in AI systems and the general responses to date. It then investigates the production of truth in InstructGPT, a large language model, highlighting how data harvesting, model architectures, and social feedback mechanisms weave together disparate understandings of veracity. It conceptualizes this performance as an operationalization of truth, where distinct, often conflicting claims are smoothly synthesized and confidently presented into truth-statements. We argue that these same logics and inconsistencies play out in Instruct’s successor, ChatGPT, reiterating truth as a non-trivial problem. We suggest that enriching sociality and thickening “reality” are two promising vectors for enhancing the truth-evaluating capacities of future language models. We conclude, however, by stepping back to consider AI truth-telling as a social practice: what kind of “truth” do we as listeners desire? Keywords— truthfulness, veracity, AI, large language model, GPT-3, InstructGPT, ChatGPT 1 ar X iv :2 30 1. 12 06 6v 1 [ cs .C Y ] 2 8 Ja n 20 23 ChatGPT was released with great fanfare in December 2022. OpenAI’s latest language model appeared to be powerful and almost magical, generating news articles, writing poetry, and explaining arcane concepts instantly and on demand. But a week later, the coding site StackOverflow banned all answers produced by the model. “The primary problem,” explained the staff, “is that while the answers which ChatGPT produces have a high rate of being incorrect, they typically look like they might be good and the answers are very easy to produce” (Vincent 2022). For a site aiming to provide correct answers to coding problems, the issue was clear: the AI model was “substantially harmful.” As AI technologies are rolled out into healthcare, academia, human resources, law, and a multitude of other domains, they become de-facto arbiters of truth. Researchers have suggested that vulnerabilities in these models could be deployed by malicious actors to produce misinformation rapidly and at scale (Dhanjani 2021; Weidinger et.al. 2022). But more concerning is the everyday impact of this dependence on automated truth claims. For instance, incorrect advice on medical symptoms and drugs can lead to patient harm or death (Bickmore et al. 2018), with one medical chatbot based on GPT-3 already advising a patient to kill themselves (Quach 2022). Whether in medicine or other domains, belief in the oftenplausible claims of these AI oracles can lead to unwarranted trust in questionable models (Passi and Vorvoreanu 2022). Such potentials increasingly proliferate with AI’s deployment across industries and social fields, testifying to the stakes of truth in AI systems. But while AI systems are increasingly given authority and invested with veracity, truth is highly contested. There are many different understandings of what truth means and how we might arrive at a truthful claim, and how truth may be verified or evaluated. No longer limited to binary notions of true or false, AI systems instead rely on degrees of truth, and may attempt to use a dataset’s implicit features, employ explicit fact checking, or appeal to authority as a method (García Lozano 2020). Osterlind (2019) suggests that quantitative methods reveal unexpected patterns, challenging old fashioned notions of fact and accuracy based on biased human assumptions. And Maruyama (2022) concludes that truth in data science may be regarded as “post-truth,” fundamentally different from truth in traditional science. Choosing an approach to truth and implementing it within a computational system is not given, but must be decided. We stress then that truth in AI is not just technical but also social, cultural, and political, drawing on particular norms and values. And yet we also recognise that the technical matters: translating truth theories into actionable architectures and processes updates them in significant ways. These disparate sociotechnical forces coalesce into a final AI model which purports to tell the truth—and in doing so, our understanding of “truth” is remade. “The ideal of truth is a fallacy for semantic interpretation and needs to be changed,” suggested two AI researchers (Welty and Aroyo 2015). This article is interested less in truth as a function of AI—how accurate a given model is, according to criteria. Rather it focuses on what the advent of AI—and specifically of language models like ChatGPT—means for the relation between truth and language. The first section discusses the contested nature of truth and the problems that it represents within AI models. The second section builds on these ideas by examining InstructGPT, an important large language model, highlighting the disparate approaches to evaluating and producing truth embedded in its social and technical layers. The third section discusses how the model synthesizes these disparate approaches into a functional machine that can generate truth claims on demand, a dynamic we term the operationalization of truth. The fourth section shows how these same logics and inconsistencies play out in Instruct’s successor, ChatGPT, reiterating once more truth as a non-trivial problem. And the fifth section suggests that enriching sociality and thickening “reality” are two promising vectors for enhancing the truth-evaluating capacities of future language models. We conclude by turning to Foucault’s Discourse and Truth (2019) to reflect on the role that these verification machines should play. If truth claims emerge from a certain arrangement of social actors and associated expectations, then these questions can be posed about language models as much as human interlocutors: what is the truth we are looking for? Risking paradox, we could ask further: what is AI’s true truth? 1. AI’s Struggle For Truth The de-facto understanding of truth in AI models is centered around “ground truth.” This is often referred to as the “fundamental truth” underpinning testing and training data or the “reality” that a developer	ArXiv	[{'authorId': '73236053', 'name': 'Luke Munn'}, {'authorId': '2733075', 'name': 'L. Magee'}, {'authorId': '121615955', 'name': 'Vanicka Arora'}]
130	c74857ae0369fbeb229795eab2f2c63765cae4dd	HPV training		British Dental Journal	[{'authorId': '51027918', 'name': 'A. Kaushik'}]
131	f0ea9e2d3889d37f34743ed1dc64f11e8e0484de	Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models	Large language models (LLM) such as OpenAI's ChatGPT and GPT-3 offer unique testbeds for exploring the translation challenges of turning literacy into numeracy. Previous publicly-available transformer models from eighteen months prior and 1000 times smaller failed to provide basic arithmetic. The statistical analysis of four complex datasets described here combines arithmetic manipulations that cannot be memorized or encoded by simple rules. The work examines whether next-token prediction succeeds from sentence completion into the realm of actual numerical understanding. For example, the work highlights cases for descriptive statistics on in-memory datasets that the LLM initially loads from memory or generates randomly using python libraries. The resulting exploratory data analysis showcases the model's capabilities to group by or pivot categorical sums, infer feature importance, derive correlations, and predict unseen test cases using linear regression. To extend the model's testable range, the research deletes and appends random rows such that recall alone cannot explain emergent numeracy.	ArXiv	[{'authorId': '46787948', 'name': 'David Noever'}, {'authorId': '2197525782', 'name': 'Forrest McKee'}]
132	cc4bac2342a3189a43fc8f63820c74e9b1584828	Chatbots as Problem Solvers: Playing Twenty Questions with Role Reversals	New chat AI applications like ChatGPT offer an advanced understanding of question context and memory across multi-step tasks, such that experiments can test its deductive reasoning. This paper proposes a multi-role and multi-step challenge, where ChatGPT plays the classic twenty-questions game but innovatively switches roles from the questioner to the answerer. The main empirical result establishes that this generation of chat applications can guess random object names in fewer than twenty questions (average, 12) and correctly guess 94% of the time across sixteen different experimental setups. The research introduces four novel cases where the chatbot fields the questions, asks the questions, both question-answer roles, and finally tries to guess appropriate contextual emotions. One task that humans typically fail but trained chat applications complete involves playing bilingual games of twenty questions (English answers to Spanish questions). Future variations address direct problem-solving using a similar inquisitive format to arrive at novel outcomes deductively, such as patentable inventions or combination thinking. Featured applications of this dialogue format include complex protein designs, neuroscience metadata, and child development educational materials.	ArXiv	[{'authorId': '46787948', 'name': 'David Noever'}, {'authorId': '2197525782', 'name': 'Forrest McKee'}]
133	974e75813699f7fba8a71b58001e4bfda9c5231b	Languages are Rewards: Hindsight Finetuning using Human Feedback	Learning from human preferences is important for language models to be helpful and useful for humans, and to align with human and social values. Existing works focus on supervised finetuning of pretrained models, based on curated model generations that are preferred by human labelers. Such works have achieved remarkable successes in understanding and following instructions (e.g., InstructGPT, ChatGPT, etc). However, to date, a key limitation of supervised finetuning is that it cannot learn from negative ratings; models are only trained on positive-rated data, which makes it data inefficient. Because collecting human feedback data is both time consuming and expensive, it is vital for the model to learn from all feedback, akin to the remarkable ability of humans to learn from diverse feedback. In this work, we propose a novel technique called Hindsight Finetuning for making language models learn from diverse human feedback. In fact, our idea is motivated by how humans learn from hindsight experience. We condition the model on a sequence of model generations paired with hindsight feedback, and finetune the model to predict the most preferred output. By doing so, models can learn to identify and correct negative attributes or errors. Applying the method to GPT-J, we observe that it significantly improves results on summarization and dialogue tasks using the same amount of human feedback.		[{'authorId': '2143856672', 'name': 'Hao Liu'}, {'authorId': '47218071', 'name': 'Carmelo Sferrazza'}, {'authorId': '1689992', 'name': 'P. Abbeel'}]
134	5c125a2d0ed00185a08c67ee64b9d29250fd5274	Better by you, better than me, chatgpt3 as writing assistance in students essays	Aim: To compare students' essay writing performance with or without employing ChatGPT-3 as a writing assistant tool. Materials and methods: Eighteen students participated in the study (nine in control and nine in the experimental group that used ChatGPT-3). We scored essay elements with grades (A-D) and corresponding numerical values (4-1). We compared essay scores to students' GPTs, writing time, authenticity, and content similarity. Results: Average grade was C for both groups; for control (2.39, SD=0.71) and for experimental (2.00, SD=0.73). None of the predictors affected essay scores: group (P=0.184), writing duration (P=0.669), module (P=0.388), and GPA (P=0.532). The text unauthenticity was slightly higher in the experimental group (11.87%, SD=13.45 to 9.96%, SD=9.81%), but the similarity among essays was generally low in the overall sample (the Jaccard similarity index ranging from 0 to 0.054). In the experimental group, AI classifier recognized more potential AI-generated texts. Conclusions: This study found no evidence that using GPT as a writing tool improves essay quality since the control group outperformed the experimental group in most parameters.		[{'authorId': '6111719', 'name': 'Ž. Bašić'}, {'authorId': '108799474', 'name': 'A. Banovac'}, {'authorId': '14293972', 'name': 'I. Kružić'}, {'authorId': '1988029', 'name': 'I. Jerković'}]
135	074baf835aec44a100990178859b35451975f339	Navigating Complexity in Software Engineering: A Prototype for Comparing GPT-n Solutions	—Navigating the diverse solution spaces of non-trivial software engineering tasks requires a combination of technical knowledge, problem-solving skills, and creativity. With multiple possible solutions available, each with its own set of trade-offs, it is essential for programmers to evaluate the various options and select the one that best suits the speciﬁc requirements and constraints of a project. Whether it is choosing from a range of libraries, weighing the pros and cons of different architecture and design solutions, or ﬁnding unique ways to fulﬁll user requirements, the ability to think creatively is crucial for making informed decisions that will result in efﬁcient and effective software. However, the interfaces of current chatbot tools for programmers, such as OpenAI’s ChatGPT or GitHub Copilot, are optimized for presenting a single solution, even for complex queries. While other solutions can be requested, they are not dis- played by default and are not intuitive to access. In this paper, we present our work-in-progress prototype “GPTC OMPARE ”, which allows programmers to visually compare multiple source code solutions generated by GPT-n models for the same programming- related query by highlighting their similarities and differences.	ArXiv	[{'authorId': '1685418', 'name': 'Christoph Treude'}]
136	672ff4a42de1bfc18b38d3d03195544715847ee1	Artificial Intelligence, Basic Skills, and Quantitative Literacy	The introduction in November 2022 of ChatGPT, a freely available language-based artificial intelligence, has led to concerns among some educators about the feasibility and benefits of teaching basic writing and critical thinking skills to students in the context of easily accessed, AI-based cheating mechanisms. As of now, ChatGPT can write pretty convincing student-level prose, but it is still not very good at answering quantitatively rich questions. Therefore, for the time being, the preceding concerns may not be shared by a large portion of the numeracy education community. However, as Google and WolframAlpha are definitely capable of answering standard and some non-standard quantitative queries, a future generation of artificial intelligence including both types of capabilities is not out of the question. So, the issue is still relevant to the readers of this journal. As we continue to focus on the higher-level skills and habits of mind that make up quantitative literacy (QL) and quantitative reasoning (QR), we should not forget that basic literacy and numeracy are still foundational building blocks. While AI is making advances in these basic realms, our human students seem to be losing ground, as implied by the most recent NAEP scores. Here we encourage our readership to focus on what makes QL/QR so challenging to teach, to human as well as artificial intelligences.	Numeracy	[{'authorId': '3099078', 'name': 'Gizem Karaali'}]
137	767b18caa5a389272bcc6966c7ba6f96113bd926	Maßnahmen gegen Hass im Netz: Das Wichtigste aus dem Herbst 2022	Aktuelle, kommentierte Ereignisse im Kampf gegen digitalen Hass. Diesmal im Rückblick auf das letzte Quartal: Zivilgesellschaftliche Kampagne gegen Antisemitismus auf TikTok +++ Verleumdungsklage gegen Twitter erfolgreich +++ Milliardenklage gegen Meta in Kenia +++ Neues Twitter-Feature macht Verbreitung von Hass nachvollziehbarer +++ ChatGPT reproduziert Stereotype +++ TikTok kündigt algorithmische Transparenz an +++ Kein TikTok für US-Regierungsangehörige +++ EU-Aufsicht für Twitter beantragt +++ DSA und Mastodon +++ Außerdem neu: Ausgewählte Forschungsarbeiten zum Thema Hass im Netz.	Machine Against the Rage	[{'authorId': '2091007360', 'name': 'Maik Fielitz'}, {'authorId': '114339363', 'name': 'Holger Marcks'}, {'authorId': '2202563157', 'name': 'Harald Sick'}, {'authorId': '2202557246', 'name': 'Hendrik Bitzmann'}]
138	73cfaadd18a48b1217ca9bf11c0e3fb8d4af6a86	Reliable Natural Language Understanding with Large Language Models and Answer Set Programming	Humans understand language by extracting information (meaning) from sentences, combining it with existing commonsense knowledge, and then performing reasoning to draw conclusions. While large language models (LLMs) such as GPT-3 and ChatGPT are able to leverage patterns in the text to solve a variety of NLP tasks, they fall short in problems that require reasoning. They also cannot reliably explain the answers generated for a given question. In order to emulate humans better, we propose STAR, a framework that combines LLMs with Answer Set Programming (ASP). We show how LLMs can be used to effectively extract knowledge -- represented as predicates -- from language. Goal-directed ASP is then employed to reliably reason over this knowledge. We apply the STAR framework to three different NLU tasks requiring reasoning: qualitative reasoning, mathematical reasoning, and goal-directed conversation. Our experiments reveal that STAR is able to bridge the gap of reasoning in NLU tasks, leading to significant performance improvements, especially for smaller LLMs, i.e., LLMs with a smaller number of parameters. NLU applications developed using the STAR framework are also explainable: along with the predicates generated, a justification in the form of a proof tree can be produced for a given output.		[{'authorId': '2153463830', 'name': 'Abhiramon Rajasekharan'}, {'authorId': '2116806486', 'name': 'Yankai Zeng'}, {'authorId': '2151051389', 'name': 'Parth Padalkar'}, {'authorId': '2146687315', 'name': 'Gopal Gupta'}]
139	1bfe1e6626067e9075570c575153e2b366f9d6cf	Could an Artificial-Intelligence agent pass an introductory physics course?	Massive pre-trained language models have garnered attention and controversy due to their ability to generate human-like responses: attention due to their frequent indistinguishability from human-generated phraseology and narratives, and controversy due to the fact that their convincingly presented arguments and facts are frequently simply false. Just how human-like are these responses when it comes to dialogues about physics, in particular about the standard content of introductory physics courses? This study explores that question by having ChatGTP, the pre-eminent language model in 2023, work through representative assessment content of an actual calculus-based physics course and grading the responses in the same way human responses would be graded. As it turns out, ChatGPT would narrowly pass this course while exhibiting many of the preconceptions and errors of a beginning learner.		[{'authorId': '3296536', 'name': 'G. Kortemeyer'}]
140	dfc66a53c836846d524765f44e111c94ebd2ad37	Impact of Artificial Intelligence on Dental Education: A Review and Guide for Curriculum Update	In this intellectual work, the clinical and educational aspects of dentistry were confronted with practical applications of artificial intelligence (AI). The aim was to provide an up-to-date overview of the upcoming changes and a brief analysis of the influential advancements in the use of AI in dental education since 2020. In addition, this review provides a guide for a dental curriculum update for undergraduate and postgraduate education in the context of advances in AI applications and their impact on dentistry. Unsurprisingly, most dental educators have limited knowledge and skills to assess AI applications, as they were not trained to do so. Also, AI technology has evolved exponentially in recent years. Factual reliability and opportunities with OpenAI Inc.’s ChatGPT are considered critical inflection points in the era of generative AI. Updating curricula at dental institutions is inevitable as advanced deep-learning approaches take over the clinical areas of dentistry and reshape diagnostics, treatment planning, management, and telemedicine screening. With recent advances in AI language models, communication with patients will change, and the foundations of dental education, including essay, thesis, or scientific paper writing, will need to adapt. However, there is a growing concern about its ethical and legal implications, and further consensus is needed for the safe and responsible implementation of AI in dental education.	Education sciences	[{'authorId': '9043564', 'name': 'Andrej Thurzo'}, {'authorId': '117988403', 'name': 'M. Strunga'}, {'authorId': '48479577', 'name': 'R. Urban'}, {'authorId': '2187955838', 'name': 'Jana Surovková'}, {'authorId': '2142706255', 'name': 'K. Afrashtehfar'}]
141	1853a05df0ac03f8e4445ec65f7599359e89fb2b	ThoughtSource: A central hub for large language model reasoning data	Large language models (LLMs) such as GPT-3 and ChatGPT have recently demonstrated impressive results across a wide range of tasks. LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to ‘hallucinate’ facts, and there are concerns about their underlying biases. Letting models verbalize reasoning steps as natural language, a technique known as chain-of-thought prompting, has recently been proposed as a way to address some of these issues. Here we present the ﬁrst release of ThoughtSource, a meta-dataset and so�ware library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to improve future artiﬁcial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data. This ﬁrst release of ThoughtSource integrates six scientiﬁc/medical, three general-domain and ﬁve math word question answering datasets.	ArXiv	"[{'authorId': '119994729', 'name': 'Simon Ott'}, {'authorId': '2203247685', 'name': 'Konstantin Hebenstreit'}, {'authorId': '2067078585', 'name': ""Valentin Li'evin""}, {'authorId': '4475692', 'name': 'C. Hother'}, {'authorId': '144334436', 'name': 'M. Moradi'}, {'authorId': '2104625194', 'name': 'Maximilian Mayrhauser'}, {'authorId': '2203245379', 'name': 'Robert Praas'}, {'authorId': '1724252', 'name': 'O. Winther'}, {'authorId': '3004898', 'name': 'M. Samwald'}]"
142	6962b1a6164e6028def3326f83d30a1bd3701d04	Why Proposal Review Should Be More Like Meteorology	"The process of evaluating research proposals for funding is often based on subjective assessments of the ""goodness"" or ""badness"" of a proposal. However, this method of evaluation is not precise and does not provide a common language for reviewers to communicate with each other. In this paper, we propose that science funding agencies ask reviewers to assign quantitative probabilities to the likelihood of a proposal reaching a particular milestone or achieving technical goals. This approach would encourage reviewers to be more precise in their evaluations and could improve both agency-wide and individual reviewer calibration over time. Additionally, this method would allow funding agencies to identify skilled reviewers and allow reviewers to improve their own performance through consistent feedback. While this method may not be suitable for all types of research, it has the potential to enhance proposal review in a variety of fields. [abstract generated by ChatGPT]"	Seeds of Science	[{'authorId': '2073872541', 'name': 'S. Buck'}]
143	680c72c29b518398d9c45b5995a160583ea8e090	Analysis of large-language model versus human performance for genetics questions	Large-language models like ChatGPT have recently received a great deal of attention. To assess ChatGPT in the field of genetics, we compared its performance to human respondents in answering genetics questions (involving 13,636 responses) that had been posted on social media platforms starting in 2021. Overall, ChatGPT did not perform significantly differently than human respondents, but did significantly better on memorization-type questions versus critical thinking questions, frequently provided different answers when asked questions multiple times, and provided plausible explanations for both correct and incorrect answers.	medRxiv	[{'authorId': '2115335067', 'name': 'D. Duong'}, {'authorId': '2110362084', 'name': 'B. D. Solomon'}]
144	c943275dc0be5d3b16c92ef450b260cef9d7c3b0	Transformers Go for the LOLs: Generating (Humourous) Titles from Scientific Abstracts End-to-End	We consider the end-to-end abstract-to-title generation problem, exploring seven recent transformer based models (including ChatGPT) ﬁne-tuned on more than 30k abstract-title pairs from NLP and machine learning venues. As an extension, we also consider the harder problem of generating humorous paper titles. For the latter, we compile the ﬁrst large-scale humor annotated dataset for scientiﬁc papers in the NLP/ML domains, comprising almost 2.5k titles. We evaluate all models using human and automatic metrics. Our human evaluation suggests that our best end-to-end system performs similarly to human authors (but arguably slightly worse). Generating funny titles is more difﬁcult, however, and our automatic systems clearly underperform relative to humans and often learn dataset artefacts of humor. Finally, ChatGPT, without any ﬁne-tuning, performs on the level of our best ﬁne-tuned system. 1	ArXiv	[{'authorId': '2109307862', 'name': 'Yanran Chen'}, {'authorId': '2620186', 'name': 'Steffen Eger'}]
145	49375b0edfaf2087ffa24a2e052f48c9922c2f09	Search for Medical Information and Treatment Options for Musculoskeletal Disorders through an Artificial Intelligence Chatbot: Focusing on Shoulder Impingement Syndrome	Background: The ChatGPT is an artificial intelligence chatbot that processes natural language text learned through reinforcement learning based on the GPT-3.5 architecture, a large-scale language model. Natural language processing models are being used in various fields and are gradually expanding their use in the medical field. Purpose: This study aimed to investigate the medical information or treatment options that ChatGPT can provide for SIS. Method: Using ChatGPT, which is provided as a free beta test, messages related to SIS were entered, and responses to medical information and treatment options were received and analyzed. Result: ChatGPT not only provided answers to the definition, prevalence, and risk factors of SIS, but also symptoms, diseases with similar symptoms, and orthopedic tests according to the messages input. Additionally, a list of treatment options and exercises were provided. Conclusion: ChatGPT will be able to provide overall useful medical information and treatment options to patients unfamiliar with SIS. However, caution is required as it contains content that may be biased or inappropriate information for patients with SIS. Nevertheless, if natural language processing technology develops further, it is expected to be able to express more detailed medical information and treatment options.	medRxiv	[{'authorId': '103525980', 'name': 'J. Kim'}]
146	8a5c9f0606b985dee71b6d8417023805f0938a9f	Are Deep Neural Networks SMARTer than Second Graders?	Recent times have witnessed an increasing number of applications of deep neural networks towards solving tasks that require superior cognitive abilities, e.g., playing Go, generating art, question answering (such as ChatGPT), etc. Such a dramatic progress raises the question: how generalizable are neural networks in solving problems that demand broad skills? To answer this question, we propose SMART: a Simple Multimodal Algorithmic Reasoning Task and the associated SMART-101 dataset, for evaluating the abstraction, deduction, and generalization abilities of neural networks in solving visuo-linguistic puzzles designed speciﬁcally for children in the 6–8 age group. Our dataset consists of 101 unique puzzles; each puzzle comprises a picture and a question, and their solution needs a mix of several elementary skills, including arithmetic, algebra, and spatial reasoning, among others. To scale our dataset towards training deep neural networks, we programmatically generate entirely new instances for each puzzle, while retaining their solution algorithm. To benchmark the performance on the SMART-101 dataset, we propose a vision and language meta-learning model using varied state-of-the-art backbone networks. Our experiments reveal that while powerful deep models offer reasonable performances on puzzles that they are trained on, they are not better than random accuracy when analyzed for generalization. We also evaluate the recent ChatGPT large language model on a subset of our dataset and ﬁnd that while ChatGPT produces convincing reasoning abilities, the answers are often incorrect.	ArXiv	[{'authorId': '2691929', 'name': 'A. Cherian'}, {'authorId': '2692770', 'name': 'Kuan-Chuan Peng'}, {'authorId': '1890366', 'name': 'Suhas Lohit'}, {'authorId': '34771975', 'name': 'Kevin A. Smith'}, {'authorId': '1763295', 'name': 'J. Tenenbaum'}]
147	905c886090f1943da1e44ab2ece7e7659cf5a35c	The Turing Deception	"This research revisits the classic Turing test and compares recent large language models such as ChatGPT for their abilities to reproduce human-level comprehension and compelling text generation. Two task challenges- summary and question answering- prompt ChatGPT to produce original content (98-99%) from a single text entry and sequential questions initially posed by Turing in 1950. We score the original and generated content against the OpenAI GPT-2 Output Detector from 2019, and establish multiple cases where the generated content proves original and undetectable (98%). The question of a machine fooling a human judge recedes in this work relative to the question of ""how would one prove it?"" The original contribution of the work presents a metric and simple grammatical set for understanding the writing mechanics of chatbots in evaluating their readability and statistical clarity, engagement, delivery, overall quality, and plagiarism risks. While Turing's original prose scores at least 14% below the machine-generated output, whether an algorithm displays hints of Turing's true initial thoughts (the ""Lovelace 2.0"" test) remains unanswerable."	ArXiv	[{'authorId': '46787948', 'name': 'David Noever'}, {'authorId': '1866333313', 'name': 'Matt Ciolino'}]
148	071fb0b7af9b3d390c7aa64a26068e23a81c52d7	Paraphrase Identification with Deep Learning: A Review of Datasets and Methods	The rapid advancement of AI technology has made text generation tools like GPT-3 and ChatGPT increasingly accessible, scalable, and effective. This can pose serious threat to the credibility of various forms of media if these technologies are used for plagiarism, including scientiﬁc literature and news sources. Despite the development of automated methods for paraphrase identiﬁcation, detecting this type of plagiarism remains a challenge due to the disparate nature of the datasets on which these methods are trained. In this study, we review traditional and current approaches to paraphrase identiﬁcation and propose a reﬁned typology of paraphrases. We also investigate how this typology is represented in popular datasets and how under-representation of certain types of paraphrases impacts detection capabilities. Finally, we outline new directions for future research and datasets in the pursuit of more effective paraphrase detection using AI.	ArXiv	[{'authorId': '2110844037', 'name': 'Chao Zhou'}, {'authorId': '2125259032', 'name': 'Cheng Qiu'}, {'authorId': '25636121', 'name': 'Daniel Ernesto Acuna'}]
149	11ddb0953eae196dab339bfdc117221594cf945e	ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models	State-of-the-art poetry generation systems are often complex. They either consist of task-speciﬁc model pipelines, incorporate prior knowledge in the form of manually created constraints or both. In contrast, end-to-end models would not suﬀer from the overhead of having to model prior knowledge and could learn the nuances of poetry from data alone, reducing the degree of human supervision required. In this work, we investigate end-to-end poetry generation conditioned on styles such as rhyme, meter, and alliteration. We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts. In particular, we successfully pre-train and release ByGPT5, a new token-free decoder-only language model, and ﬁne-tune it on a large custom corpus of English and German quatrains annotated with our styles. We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and ChatGPT, while also being more parameter eﬃcient and performing favorably compared to humans. In addition, we analyze its runtime performance and introspect the model’s understanding of style conditions. We make our code, models, and datasets publicly available. 1	ArXiv	[{'authorId': '2138207755', 'name': 'Jonas Belouadi'}, {'authorId': '2620186', 'name': 'Steffen Eger'}]
150	51c1ec9e470f73d48e23816e4cdec9cc0adfef14	The Implications of OpenAI’s Assistant for Legal Services and Society		SSRN Electronic Journal	"[{'authorId': '2197932431', 'name': ""Open AI's Assistant ChatGPT""}, {'authorId': '50350095', 'name': 'Andrew M. Perlman'}]"
151	9aed848be4e9e401b0e61a2e5d60dbdafa0c6cc1	A Dataset and Benchmark for Automatically Answering and Generating Machine Learning Final Exams	Can a machine learn machine learning? We propose to answer this question using the same criteria we use to answer a similar question: can a human learn machine learning? We automatically answer ﬁnal exams in MIT’s, Harvard’s and Cornell’s large machine learning courses and generate new questions at a human level. Recently, program synthesis and few-shot learning solved university-level problem set questions in mathematics and STEM courses at a human level. In this work, we solve questions from ﬁnal exams that differ from problem sets in several ways: the questions are longer, have multiple parts, are more complicated, and span a broader set of topics. We provide a new dataset and benchmark of questions from machine learning ﬁnal exams and code for automatically answering these questions and generating new questions. To make our dataset a reproducible benchmark, we use automatic checkers for multiple choice questions, questions with numeric answers, and questions with expression answers, and evaluate a large free language model, Meta’s OPT, and compare the results with Open AI’s GPT-3, ChatGPT, and Codex. A student survey comparing the quality, appropriateness, and difﬁculty of machine-generated questions with human-written questions shows that across multiple aspects, machine-generated questions are indistinguishable from human-generated questions and are suitable for ﬁnal exams. We perform ablation studies comparing zero-shot learning with few-shot learning, chain-of-thought prompting, GPT-3, ChatGPT, and OPT pre-trained on text and Codex ﬁne-tuned on code on a range of machine learning topics and ﬁnd that few-shot learning methods perform best. We make our data and code publicly available for the machine learning community	ArXiv	[{'authorId': '2161720923', 'name': 'Sarah Zhang'}, {'authorId': '2161564675', 'name': 'Reece Shuttleworth'}, {'authorId': '2060771111', 'name': 'Derek Austin'}, {'authorId': '2170071419', 'name': 'Yann Hicke'}, {'authorId': '2144484107', 'name': 'Leonard Tang'}, {'authorId': '2170072533', 'name': 'Sathwik Karnik'}, {'authorId': '2170073076', 'name': 'Darnell Granberry'}, {'authorId': '2861627', 'name': 'Iddo Drori'}]
152	9422fab4f79d16c79fce6d4da7889ad832342f6d	AI-enabled transformations in telecommunications industry		Telecommunications Systems	[{'authorId': '2115776021', 'name': 'Muhammad Khurram Khan'}]
153	a23e0cf74f10b6a3061ab71497fe2c8c476fecd1	Conversational Automated Program Repair	Automated Program Repair (APR) can help developers automatically generate patches for bugs. Due to the impressive performance obtained using Large PreTrained Language Models (LLMs) on many code related tasks, researchers have started to directly use LLMs for APR. However, prior approaches simply repeatedly sample the LLM given the same constructed input/prompt created from the original buggy code, which not only leads to generating the same incorrect patches repeatedly but also miss the critical information in testcases. To address these limitations, we propose conversational APR, a new paradigm for program repair that alternates between patch generation and validation in a conversational manner. In conversational APR, we iteratively build the input to the model by combining previously generated patches with validation feedback. As such, we leverage the long-term context window of LLMs to not only avoid generating previously incorrect patches but also incorporate validation feedback to help the model understand the semantic meaning of the program under test. We evaluate 10 different LLM including the newly developed ChatGPT model to demonstrate the improvement of conversational APR over the prior LLM for APR approach.	ArXiv	[{'authorId': '145349987', 'name': 'Chun Xia'}, {'authorId': '2145398332', 'name': 'Lingming Zhang'}]
154	3883e49fc62b1f867d19f6550924126783ee2376	Modeling Label Semantics Improves Activity Recognition	Human activity recognition (HAR) aims to classify sensory time series into different activities, with wide applications in activity tracking, healthcare, human computer interaction, etc. Existing HAR works improve recognition performance by designing more complicated feature extraction methods, but they neglect the label semantics by simply treating labels as integer IDs. We find that many activities in the current HAR datasets have shared label names, e.g., “open door” and “open fridge”, “walk upstairs” and “walk downstairs”. Through some exploratory analysis, we find that such shared structure in activity names also maps to similarity in the input features. To this end, we design a sequence-to-sequence framework to decode the label name semantics rather than classifying labels as integer IDs. Our proposed method decomposes learning activities into learning shared tokens (“open”, “walk”), which is easier than learning the joint distribution (“open fridge”, “walk upstairs”) and helps transfer learning to activities with insufficient data samples. For datasets originally without shared tokens in label names, we also offer an automated method, using OpenAI’s ChatGPT, to generate shared actions and objects. Extensive experiments on seven HAR benchmark datasets demonstrate the state-of-the-art performance of our method. We also show better performance in the long-tail activity distribution settings and few-shot settings.	ArXiv	[{'authorId': '2108217022', 'name': 'Xiyuan Zhang'}, {'authorId': '134629191', 'name': 'Ranak Roy Chowdhury'}, {'authorId': '2505157', 'name': 'Dezhi Hong'}, {'authorId': '2110343779', 'name': 'Rajesh K. Gupta'}, {'authorId': '2163679367', 'name': 'Jingbo Shang'}]
155	b5e339086e954112c2c6905cb18975a769d0f2b9	Deep Machine Learning in Cosmology: Evolution or Revolution?	Could Machine Learning (ML) make fundamental discoveries and tackle unsolved problems in Cosmology? Detailed observations of the present contents of the universe are consistent with the Cosmological Constant Lambda and Cold Dark Matter model, subject to some unresolved inconsistencies ('tensions') among observations of the Hubble Constant and the clumpiness factor. To understand these issues further, large surveys of billions of galaxies and other probes require new statistical approaches. In recent years the power of ML, and in particular 'Deep Learning', has been demonstrated for object classification, photometric redshifts, anomaly detection, enhanced simulations, and inference of cosmological parameters. It is argued that the more traditional 'shallow learning' (i.e. with pre-processing feature extraction) is actually quite deep, as it brings in human knowledge, while 'deep learning' might be perceived as a black box, unless supplemented by explainability tools. The 'killer applications' of ML for Cosmology are still to come. New ways to train the next generation of scientists for the Data Intensive Science challenges ahead are also discussed. Finally, the chatbot ChatGPT is challenged to address the question posed in this article's title.		[{'authorId': '103135769', 'name': 'O. Lahav'}]
156	a9c0d7bad124697e370a1b345508b6bc618a4f64	Ethics of generative AI	Artificial intelligence (AI) and its introduction into clinical pathways presents an array of ethical issues that are being discussed in the JME. The development of AI technologies that can produce text that will pass plagiarism detectors and are capable of appearing to be written by a human author present new issues for medical ethics. One set of worries concerns authorship and whether it will now be possible to know that an author or student in fact produced submitted work. That seems likely to be a general worry for secondary and higher education, as well as for all academic journals. Thus far generative AI chatbots do not seem to be able to produce a fully referenced and wellargued ethics article, but they probably could generate a blog or student essay that would be hard to detect after very minor edits. Many schools and universities have moved to online forms of assessment, and it seems likely that generative AI might cast doubt on the integrity of them and we might see a reversion to handwritten examinations as a solution. As well as these immediate and perhaps obvious ethical concerns, generative AI highlights conceptual challenges that pose more profound ethical questions. JME is committed to publishing highquality articles that further the ethical analysis of an issue within healthcare. Some of the content that the journal publishes reports empirical findings. An article that, for example, describes qualitative findings and then develops an analysis of some normative issues could not be solely authored by generative AI: it cannot do qualitative research. However, generative AI can find publicly available sources and produce ethical arguments and syllogisms. What does that imply about the nature of ethical analysis? If ethical analysis, fundamentally, involves assembling, organising and evaluating words, then perhaps generative AI could replace ethicists. At present, generative AI cannot produce the nuance, depth or originality of a quality ethics article, but it may be a matter of time before they pass a medical ethics version of the Turing test. While those who rely on more analytic approaches to ethics might view this as an ethical apocalypse, there are more subtler ways in which generative AI might be used in authorship that are more positive. For instance, if you experiment with ChatGPT for a while, you might find that when you know what you want to say in a paragraph or subsection, you can prompt it to form the argument you want to make and it will generate a fairly welldrafted paragraph. For authors and for postgraduate students, this might be useful for ‘throat clearing’ sections which are just laying the terrain for the reader before proceeding. AI chatbots might also play a helpful devil’s advocate: once you make a point in a paragraph, you can ask it to generate a rebuttal. If you experiment with ChatGPT you might find that it can be helpful in that not only does it generate obvious counters, but it often raises things that you might not have immediately thought of. So perhaps generative AI has the potential to pose questions like those that might be raised at a seminar while a paper or book is being refined. Much of the work involved in writing good, innovative and original ethical analysis involves wrestling with high level ideas. If generative AI can aid authors in drafting articles then perhaps they save intellectual effort that could be directed toward the ‘big picture’? Many publishers, including the BMJ, are committed to encouraging authors from the Global South to write for journals where the authorship has been primarily from the Global North. Publishing in journals such as the JME can be more difficult when English is not an author’s first language. Because of the speed at which generative AI can present an author’s ideas in what is close to idiomatic English, they have the potential to significantly open up authorship, especially for humanities style journals like the JME. Journal and copy editors might also save time and effort when correcting articles. These potential benefits will no doubt come with tradeoffs. For instance, those who struggle to articulate a point in writing may find that to be part of a process that leads to a new insight. Perhaps generative AI runs the risk of making that part of the writing process too easy and lead to missing out on opportunities for insight. While that seems like a valid worry, it might be that this is analogous to the changes in writing that resulted from journals being readily available online. Those of us who are old enough to recall writing before online publishing will have spent time trudging between library stacks and searching hard copies of journals to find a paper and stumbling across other papers and journals, which may have led to new ideas. Complete reliance on online databases has probably reduced those moments but the tradeoff clearly still favours their use. Universities, publishers and journals are likely to be exercised about what this will mean for authorship. How can we know whether work was created by the author or by generative AI? While understandable, these worries might be overstated given what they can do at present. While they might find some publicly available sources to support claims, at present they cannot adequately cite, and the quality of the content they produce is wholly dependent on you asking the right questions or having the right ideas. At present, they can be seen as a very sophisticated thesaurus and we do not worry about authors using those. Perhaps the biggest concern is that some predatory journals may feed off the speed by which lowquality manuscripts can now be generated with AI chatbots, and use the phenomenon to publish huge numbers of mostly useless or misleading ethical analyses. This could flood the journal market and undermine trust in research publications. This may be curbed by introducing the use of AI output detectors, though it may also encourage greater (and much needed) scepticism towards publications, with readers and news writers paying greater attention to where the paper they are reporting on was published, and what the publisher’s standards are. Editorially, journals need to and will continue to be concerned with authorship, but our main focus is on the quality and originality of ideas. It seems likely that generative AI is here to stay and will develop, so journals will need to find ways of figuring out how to work with them. Bioethics Centre, The University of Otago, Dunedin, New Zealand	Journal of Medical Ethics	[{'authorId': '14907572', 'name': 'Hazem Zohny'}, {'authorId': '2086699077', 'name': 'J. McMillan'}, {'authorId': '2070836888', 'name': 'M. King'}]
157	ed9fdeb7ec78a69e1ef18b8cdcabc67804a0e188	What would Harry say? Building Dialogue Agents for Characters in a Story	We have a Christmas gift for Harry Potter fans all over the world. In this paper, we present Harry Potter Dialogue (HPD), a dataset that helps train Harry Potter-like dialogue agents. Such a task is typically viewed as a variant of personalized dialogue agents, but they dif-fer signiﬁcantly in three respects: 1) Harry lived in a virtual world of wizards, thus, real-world commonsense may not apply to Harry’s conversations; 2) Harry’s behavior is strongly linked to background information in conversations: the scene, its attributes and its relationship to other speakers; and 3) Such back-grounds are dynamically altered as the storyline goes on. The HPD dataset, as the ﬁrst dataset to facilitate the study of dialogue agent construction for characters within a story, provides rich contextual information about each dialogue session such as scenes, character attributes, and relations. More importantly, all the background information will change over the course of the story. In addition, HPD could support both dialogue generation and retrieval tasks. We evaluate baselines such as Dialog-GPT and BOB to determine the extent to which they can generate Harry Potter-like responses. The experimental results disappoint us in that although the generated responses are ﬂuent, they still seem out of character for Harry. Besides, we validate the current most robust dialogue agent, ChatGPT, which also can’t generate plausible Harry-Potter-like responses in some cases, either. Our results sug-gest that there is much scope for future research.	ArXiv	[{'authorId': '119895609', 'name': 'Nuo Chen'}, {'authorId': '2152549125', 'name': 'Yan Wang'}, {'authorId': '48579460', 'name': 'Haiyun Jiang'}, {'authorId': '1724421', 'name': 'Deng Cai'}, {'authorId': '2190791544', 'name': 'Ziyang Chen'}, {'authorId': '2118373531', 'name': 'Jia Li'}]
